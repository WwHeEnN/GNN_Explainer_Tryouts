{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:34:16.163232Z",
     "iopub.status.busy": "2024-05-08T06:34:16.162670Z",
     "iopub.status.idle": "2024-05-08T06:34:18.473000Z",
     "shell.execute_reply": "2024-05-08T06:34:18.472707Z",
     "shell.execute_reply.started": "2024-05-08T06:34:16.163195Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# import torchvision\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "# Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "# !pip install --quiet pytorch-lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:41:54.448879Z",
     "iopub.status.busy": "2024-05-08T06:41:54.448405Z",
     "iopub.status.idle": "2024-05-08T06:41:55.673864Z",
     "shell.execute_reply": "2024-05-08T06:41:55.673589Z",
     "shell.execute_reply.started": "2024-05-08T06:41:54.448846Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/hy0mnzl536ng5c4qz99ppmf00000gn/T/ipykernel_7595/1179797987.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:09.408805Z",
     "iopub.status.busy": "2024-05-08T06:42:09.408577Z",
     "iopub.status.idle": "2024-05-08T06:42:09.411991Z",
     "shell.execute_reply": "2024-05-08T06:42:09.411740Z",
     "shell.execute_reply.started": "2024-05-08T06:42:09.408793Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:09.412569Z",
     "iopub.status.busy": "2024-05-08T06:42:09.412438Z",
     "iopub.status.idle": "2024-05-08T06:42:09.415445Z",
     "shell.execute_reply": "2024-05-08T06:42:09.414919Z",
     "shell.execute_reply.started": "2024-05-08T06:42:09.412559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936],\n",
      "        [0.8694, 0.5677, 0.7411]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:11.838524Z",
     "iopub.status.busy": "2024-05-08T06:42:11.837873Z",
     "iopub.status.idle": "2024-05-08T06:42:12.001401Z",
     "shell.execute_reply": "2024-05-08T06:42:12.001087Z",
     "shell.execute_reply.started": "2024-05-08T06:42:11.838475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:13.580929Z",
     "iopub.status.busy": "2024-05-08T06:42:13.579876Z",
     "iopub.status.idle": "2024-05-08T06:42:13.588564Z",
     "shell.execute_reply": "2024-05-08T06:42:13.586682Z",
     "shell.execute_reply.started": "2024-05-08T06:42:13.580856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check that MPS is available\n",
    "#  not in the original code \n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:17.457308Z",
     "iopub.status.busy": "2024-05-08T06:42:17.456738Z",
     "iopub.status.idle": "2024-05-08T06:42:17.831181Z",
     "shell.execute_reply": "2024-05-08T06:42:17.830885Z",
     "shell.execute_reply.started": "2024-05-08T06:42:17.457266Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import sort_edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a few pre-trained models we download below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:18.981972Z",
     "iopub.status.busy": "2024-05-08T06:42:18.980154Z",
     "iopub.status.idle": "2024-05-08T06:42:18.995736Z",
     "shell.execute_reply": "2024-05-08T06:42:18.994002Z",
     "shell.execute_reply.started": "2024-05-08T06:42:18.981899Z"
    }
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# from urllib.error import HTTPError\n",
    "# # Github URL where saved models are stored for this tutorial\n",
    "# base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/\"\n",
    "# # Files to download\n",
    "# pretrained_files = [\"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\", \"GraphLevelGraphConv.ckpt\"]\n",
    "\n",
    "# # Create checkpoint path if it doesn't exist yet\n",
    "# os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# # For each file, check whether it already exists. If not, try downloading it.\n",
    "# for file_name in pretrained_files:\n",
    "#     file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "#     if \"/\" in file_name:\n",
    "#         os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "#     if not os.path.isfile(file_path):\n",
    "#         file_url = base_url + file_name\n",
    "#         print(f\"Downloading {file_url}...\")\n",
    "#         try:\n",
    "#             urllib.request.urlretrieve(file_url, file_path)\n",
    "#         except HTTPError as e:\n",
    "#             print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:25.757041Z",
     "iopub.status.busy": "2024-05-08T06:42:25.756520Z",
     "iopub.status.idle": "2024-05-08T06:42:25.762516Z",
     "shell.execute_reply": "2024-05-08T06:42:25.762060Z",
     "shell.execute_reply.started": "2024-05-08T06:42:25.757007Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module): # neural network base \n",
    "    \n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out) # linear layer \n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. \n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats) # nn.Linear \n",
    "        node_feats = torch.bmm(adj_matrix, node_feats) # matrix product \n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:25.763878Z",
     "iopub.status.busy": "2024-05-08T06:42:25.763753Z",
     "iopub.status.idle": "2024-05-08T06:42:25.767227Z",
     "shell.execute_reply": "2024-05-08T06:42:25.766911Z",
     "shell.execute_reply.started": "2024-05-08T06:42:25.763866Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCNLayer_toy(nn.Module): # neural network base \n",
    "    \n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out) # linear layer \n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. \n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats) # nn.Linear \n",
    "        node_feats = torch.bmm(adj_matrix, node_feats) # matrix product \n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:25.817248Z",
     "iopub.status.busy": "2024-05-08T06:42:25.816711Z",
     "iopub.status.idle": "2024-05-08T06:42:25.844091Z",
     "shell.execute_reply": "2024-05-08T06:42:25.843477Z",
     "shell.execute_reply.started": "2024-05-08T06:42:25.817035Z"
    }
   },
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The \n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha - Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "        \n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "        \n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "        \n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1) # reshape \n",
    "        \n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix \n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False) # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0) \n",
    "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "        \n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a) # sum product of matrices \n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "        \n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15) # why is there a negative number to fill \n",
    "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "        \n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "        \n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "        \n",
    "        return node_feats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.071115Z",
     "iopub.status.busy": "2024-05-08T06:42:31.070935Z",
     "iopub.status.idle": "2024-05-08T06:42:31.077620Z",
     "shell.execute_reply": "2024-05-08T06:42:31.076938Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.071089Z"
    }
   },
   "outputs": [],
   "source": [
    "# from typing import Optional\n",
    "\n",
    "# import torch\n",
    "# from torch import Tensor\n",
    "\n",
    "# @torch.jit.script\n",
    "# def softmax(src: Tensor, index: Optional[Tensor] = None,\n",
    "#             ptr: Optional[Tensor] = None, num_nodes: Optional[int] = None,\n",
    "#             dim: int = 0) -> Tensor:\n",
    "#     return src\n",
    "# import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.082212Z",
     "iopub.status.busy": "2024-05-08T06:42:31.080545Z",
     "iopub.status.idle": "2024-05-08T06:42:31.098430Z",
     "shell.execute_reply": "2024-05-08T06:42:31.097719Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.082116Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch geometric\n",
    "try: \n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    # Installing torch geometric packages with specific CUDA+PyTorch version. \n",
    "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details \n",
    "    TORCH = torch.__version__.split('+')[0]\n",
    "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
    "\n",
    "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-geometric \n",
    "    import torch_geometric\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.data as geom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.099347Z",
     "iopub.status.busy": "2024-05-08T06:42:31.099101Z",
     "iopub.status.idle": "2024-05-08T06:42:31.101807Z",
     "shell.execute_reply": "2024-05-08T06:42:31.101435Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.099323Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.data as geom_data\n",
    "gnn_layer_by_name = {\n",
    "    \"GCN\": geom_nn.GCNConv,\n",
    "    \"GAT\": geom_nn.GATConv,\n",
    "    \"GIN\": geom_nn.GINConv,\n",
    "    \"GraphConv\": geom_nn.GraphConv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.104881Z",
     "iopub.status.busy": "2024-05-08T06:42:31.104753Z",
     "iopub.status.idle": "2024-05-08T06:42:31.106873Z",
     "shell.execute_reply": "2024-05-08T06:42:31.106411Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.104869Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx, from_networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-14T05:34:36.767774Z",
     "iopub.status.busy": "2024-02-14T05:34:36.767678Z",
     "iopub.status.idle": "2024-02-14T05:34:37.160020Z",
     "shell.execute_reply": "2024-02-14T05:34:37.158457Z",
     "shell.execute_reply.started": "2024-02-14T05:34:36.767763Z"
    }
   },
   "source": [
    "G = to_networkx(cora_dataset[0])\n",
    "degrees = [val for (node, val) in G.degree()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.107584Z",
     "iopub.status.busy": "2024-05-08T06:42:31.107444Z",
     "iopub.status.idle": "2024-05-08T06:42:31.115771Z",
     "shell.execute_reply": "2024-05-08T06:42:31.114602Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.107568Z"
    }
   },
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GraphConv\", dp_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of \"hidden\" graph layers\n",
    "            layer_name - String of the graph layer to use\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "        \n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels, \n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(inplace=True)\n",
    "                # ,nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels, \n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.117790Z",
     "iopub.status.busy": "2024-05-08T06:42:31.117223Z",
     "iopub.status.idle": "2024-05-08T06:42:31.125690Z",
     "shell.execute_reply": "2024-05-08T06:42:31.124540Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.117774Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of hidden layers\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                nn.Linear(in_channels, out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [nn.Linear(in_channels, c_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.127373Z",
     "iopub.status.busy": "2024-05-08T06:42:31.126924Z",
     "iopub.status.idle": "2024-05-08T06:42:31.134706Z",
     "shell.execute_reply": "2024-05-08T06:42:31.133721Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.127268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Small function for printing the test scores\n",
    "def print_results(result_dict):\n",
    "    if \"train\" in result_dict:\n",
    "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
    "    if \"val\" in result_dict:\n",
    "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
    "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of output features (usually number of classes)\n",
    "            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
    "            kwargs - Additional arguments for the GNNModel object\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.GNN = GNNModel(c_in=c_in, \n",
    "                            c_hidden=c_hidden, \n",
    "                            c_out=c_hidden, # Not our prediction output yet!\n",
    "                            **kwargs)\n",
    "        self.head = nn.Sequential(\n",
    "            # nn.Dropout(dp_rate_linear),\n",
    "            nn.Linear(c_hidden, c_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "            batch_idx - Index of batch element for each node\n",
    "        \"\"\"\n",
    "        x = self.GNN(x, edge_index)\n",
    "        # x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling\n",
    "        x = geom_nn.global_max_pool(x, batch_idx) # Average pooling\n",
    "        # x = geom_nn.global_add_pool(x, batch_idx) # sum pooling\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLevelGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = GraphGNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "        x = self.model(x, edge_index, batch_idx)\n",
    "        x = x.squeeze(dim=-1)\n",
    "        \n",
    "        if self.hparams.c_out == 1:\n",
    "            preds = (x > 0).float()\n",
    "            try: \n",
    "                data.y = data.y.float()\n",
    "            except: pass\n",
    "        else:\n",
    "            preds = x.argmax(dim=-1)\n",
    "        try: \n",
    "            loss = self.loss_module(x, data.y)\n",
    "            acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "        except:\n",
    "            loss = 0\n",
    "            acc = 0\n",
    "\n",
    "        return loss,acc,preds\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0) # High lr because of small dataset and small model\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc,_ = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     _, acc,preds = self.forward(batch, mode=\"val\")\n",
    "    #     self.log('val_acc', acc)\n",
    "    #     # self.log('val_pred', preds)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc,_ = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)\n",
    "        # self.log('test_pred', pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph-level tasks: Graph classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the max-degree threshold \n",
    "thres=6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.135564Z",
     "iopub.status.busy": "2024-05-08T06:42:31.135439Z",
     "iopub.status.idle": "2024-05-08T06:42:31.145297Z",
     "shell.execute_reply": "2024-05-08T06:42:31.144938Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.135541Z"
    }
   },
   "outputs": [],
   "source": [
    "tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"ENZYMES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.145898Z",
     "iopub.status.busy": "2024-05-08T06:42:31.145805Z",
     "iopub.status.idle": "2024-05-08T06:42:31.150837Z",
     "shell.execute_reply": "2024-05-08T06:42:31.150483Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.145888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,\n",
       "          3,  4,  4,  4,  4,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,\n",
       "          7,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11, 12,\n",
       "         12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16,\n",
       "         16, 16, 17, 17, 17, 17, 18, 18, 18, 19, 19, 19, 20, 20, 20, 20, 20, 20,\n",
       "         21, 21, 21, 21, 21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25,\n",
       "         25, 25, 25, 25, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 28, 28, 28, 28,\n",
       "         28, 28, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 31, 31, 31, 32,\n",
       "         32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 35, 35, 35,\n",
       "         35, 35, 36, 36, 36, 36],\n",
       "        [ 1,  2,  3,  0,  2,  3, 24, 27,  0,  1,  3, 27, 28,  0,  1,  2,  4,  5,\n",
       "         28,  3,  5,  6, 29,  3,  4,  6,  7, 29,  4,  5,  7,  8,  5,  6,  8,  9,\n",
       "         10,  6,  7,  9,  7,  8, 10, 11, 12,  7,  9, 11, 12,  9, 10, 12, 26,  9,\n",
       "         10, 11, 25, 26, 14, 15, 16, 25, 13, 15, 16, 25, 13, 14, 16, 17, 13, 14,\n",
       "         15, 17, 15, 16, 18, 19, 17, 19, 20, 17, 18, 20, 18, 19, 21, 22, 23, 30,\n",
       "         20, 22, 23, 30, 35, 20, 21, 23, 35, 20, 21, 22, 33,  1, 27, 28, 29, 12,\n",
       "         13, 14, 26, 29, 11, 12, 25, 28, 29,  1,  2, 24, 28, 29,  2,  3, 24, 26,\n",
       "         27, 29,  4,  5, 24, 25, 26, 27, 28, 20, 21, 33, 34, 35, 32, 34, 36, 31,\n",
       "         33, 34, 36, 23, 30, 32, 34, 35, 36, 30, 31, 32, 33, 35, 36, 21, 22, 30,\n",
       "         33, 34, 31, 32, 33, 34]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu_dataset[0].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.151784Z",
     "iopub.status.busy": "2024-05-08T06:42:31.151594Z",
     "iopub.status.idle": "2024-05-08T06:42:31.162338Z",
     "shell.execute_reply": "2024-05-08T06:42:31.161771Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.151770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[19580, 3], edge_index=[2, 74564], y=[600])\n",
      "Length: 600\n",
      "Average label: 2.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data object:\", tu_dataset.data)\n",
    "print(\"Length:\", len(tu_dataset))\n",
    "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.164596Z",
     "iopub.status.busy": "2024-05-08T06:42:31.163619Z",
     "iopub.status.idle": "2024-05-08T06:42:31.170715Z",
     "shell.execute_reply": "2024-05-08T06:42:31.170400Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.164488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu_dataset.data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.176998Z",
     "iopub.status.busy": "2024-05-08T06:42:31.176902Z",
     "iopub.status.idle": "2024-05-08T06:42:31.181604Z",
     "shell.execute_reply": "2024-05-08T06:42:31.180977Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.176987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu_dataset.data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.185927Z",
     "iopub.status.busy": "2024-05-08T06:42:31.184182Z",
     "iopub.status.idle": "2024-05-08T06:42:31.263386Z",
     "shell.execute_reply": "2024-05-08T06:42:31.262973Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.185902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.data.data.Data"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu2=tu_dataset.copy()\n",
    "type(tu2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.268693Z",
     "iopub.status.busy": "2024-05-08T06:42:31.268465Z",
     "iopub.status.idle": "2024-05-08T06:42:31.271546Z",
     "shell.execute_reply": "2024-05-08T06:42:31.271077Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.268676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tu2.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.318264Z",
     "iopub.status.busy": "2024-05-08T06:42:31.318070Z",
     "iopub.status.idle": "2024-05-08T06:42:31.452719Z",
     "shell.execute_reply": "2024-05-08T06:42:31.452102Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.318216Z"
    }
   },
   "outputs": [],
   "source": [
    "# update label to be 0 or 1 depends on whether it exceeds the threshold \n",
    "for i in range(len(tu2.y)): \n",
    "    tu2.y[i]=max(torch.bincount(tu2[i].edge_index[0,:]))\n",
    "    # tu2.y[i]=sum(torch.bincount(tu2[i].edge_index[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.453580Z",
     "iopub.status.busy": "2024-05-08T06:42:31.453403Z",
     "iopub.status.idle": "2024-05-08T06:42:31.471269Z",
     "shell.execute_reply": "2024-05-08T06:42:31.470577Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.453566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 6, 6, 5, 6, 6, 7, 8, 5, 5, 3, 6, 5, 5, 5, 6, 7, 7, 1, 7, 7, 6, 6, 6,\n",
       "        6, 7, 7, 6, 6, 6, 7, 7, 7, 5, 5, 8, 7, 2, 6, 7, 7, 6, 6, 6, 6, 7, 5, 5,\n",
       "        5, 4, 6, 7, 6, 6, 6, 7, 5, 6, 5, 5, 4, 6, 5, 6, 5, 5, 5, 6, 6, 5, 7, 5,\n",
       "        6, 6, 6, 6, 5, 5, 5, 5, 6, 5, 5, 5, 5, 6, 6, 7, 8, 5, 7, 7, 5, 5, 5, 5,\n",
       "        7, 6, 6, 4, 7, 6, 9, 5, 7, 5, 8, 8, 6, 5, 6, 6, 6, 5, 5, 5, 8, 5, 5, 7,\n",
       "        9, 6, 9, 6, 7, 5, 5, 6, 5, 5, 5, 6, 5, 6, 7, 2, 7, 7, 7, 5, 6, 5, 6, 6,\n",
       "        5, 7, 6, 6, 6, 5, 6, 7, 6, 4, 7, 5, 5, 5, 7, 7, 6, 5, 5, 7, 5, 5, 7, 6,\n",
       "        6, 5, 7, 5, 6, 6, 6, 6, 8, 6, 7, 7, 7, 6, 6, 6, 6, 5, 6, 5, 7, 6, 6, 6,\n",
       "        6, 6, 7, 8, 7, 5, 6, 6, 5, 5, 6, 6, 6, 6, 5, 7, 6, 6, 7, 7, 7, 8, 7, 6,\n",
       "        6, 8, 7, 6, 6, 5, 6, 8, 5, 7, 7, 7, 7, 7, 6, 6, 8, 6, 6, 6, 5, 7, 7, 7,\n",
       "        5, 5, 5, 6, 7, 5, 6, 5, 5, 8, 5, 5, 6, 7, 5, 6, 5, 6, 6, 7, 7, 6, 6, 6,\n",
       "        6, 7, 7, 8, 8, 6, 6, 6, 7, 6, 6, 7, 6, 6, 6, 5, 7, 6, 5, 5, 5, 6, 7, 7,\n",
       "        6, 6, 6, 6, 6, 6, 5, 5, 7, 6, 5, 7, 7, 8, 7, 8, 7, 6, 6, 8, 8, 8, 6, 7,\n",
       "        8, 5, 7, 7, 6, 7, 5, 6, 6, 6, 5, 5, 7, 7, 5, 5, 6, 7, 7, 7, 5, 6, 6, 5,\n",
       "        6, 6, 5, 6, 5, 5, 5, 5, 6, 5, 6, 6, 8, 6, 6, 3, 7, 7, 6, 7, 6, 6, 5, 7,\n",
       "        6, 6, 6, 7, 6, 6, 5, 5, 5, 6, 6, 5, 5, 6, 6, 5, 6, 6, 6, 5, 5, 7, 6, 6,\n",
       "        5, 6, 6, 7, 6, 6, 5, 7, 5, 6, 7, 6, 6, 5, 6, 6, 7, 7, 6, 5, 5, 6, 5, 8,\n",
       "        6, 7, 6, 6, 7, 7, 6, 6, 5, 5, 7, 6, 6, 7, 5, 7, 6, 5, 5, 6, 6, 7, 6, 5,\n",
       "        6, 5, 7, 7, 5, 9, 9, 7, 8, 9, 7, 8, 8, 7, 7, 7, 7, 8, 6, 5, 7, 8, 6, 5,\n",
       "        6, 6, 7, 7, 8, 6, 5, 7, 6, 6, 6, 6, 5, 6, 5, 6, 6, 7, 5, 5, 5, 6, 4, 5,\n",
       "        6, 5, 6, 6, 7, 6, 6, 5, 5, 6, 5, 7, 6, 5, 6, 6, 5, 5, 7, 6, 6, 6, 6, 7,\n",
       "        6, 6, 6, 6, 6, 7, 7, 7, 6, 7, 7, 6, 5, 5, 7, 7, 7, 7, 6, 5, 7, 7, 8, 7,\n",
       "        7, 5, 5, 7, 6, 6, 6, 7, 8, 7, 6, 6, 6, 7, 6, 6, 6, 6, 6, 5, 5, 5, 6, 7,\n",
       "        6, 6, 5, 5, 6, 6, 6, 6, 5, 6, 7, 6, 5, 6, 6, 7, 6, 6, 6, 5, 6, 6, 6, 5,\n",
       "        6, 6, 6, 7, 7, 6, 7, 6, 7, 6, 5, 7, 5, 6, 6, 7, 6, 8, 7, 8, 8, 7, 9, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu2.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.485250Z",
     "iopub.status.busy": "2024-05-08T06:42:31.483013Z",
     "iopub.status.idle": "2024-05-08T06:42:31.499327Z",
     "shell.execute_reply": "2024-05-08T06:42:31.498506Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.485223Z"
    }
   },
   "outputs": [],
   "source": [
    "tu2.data.y=(tu2.y>thres).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.500249Z",
     "iopub.status.busy": "2024-05-08T06:42:31.500038Z",
     "iopub.status.idle": "2024-05-08T06:42:31.511616Z",
     "shell.execute_reply": "2024-05-08T06:42:31.510902Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.500236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu2.data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.514056Z",
     "iopub.status.busy": "2024-05-08T06:42:31.513582Z",
     "iopub.status.idle": "2024-05-08T06:42:31.518953Z",
     "shell.execute_reply": "2024-05-08T06:42:31.518462Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.514011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(188), torch.Size([600]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu2.data.y.sum(),tu2.data.y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.519918Z",
     "iopub.status.busy": "2024-05-08T06:42:31.519761Z",
     "iopub.status.idle": "2024-05-08T06:42:31.522921Z",
     "shell.execute_reply": "2024-05-08T06:42:31.522413Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.519905Z"
    }
   },
   "outputs": [],
   "source": [
    "# update node features to be random gaussian \n",
    "# we dont want the GNN to pick up features, but rather using the structure of graph to do the prediciton \n",
    "tu2.data.x=tu2.x[:,:1]\n",
    "# tu2.data.x=torch.ones(tu2.x.shape) \n",
    "# assigning all node features to be 1 will confuse the model \n",
    "tu2.data.x=torch.randn(tu2.x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.523929Z",
     "iopub.status.busy": "2024-05-08T06:42:31.523583Z",
     "iopub.status.idle": "2024-05-08T06:42:31.529956Z",
     "shell.execute_reply": "2024-05-08T06:42:31.528347Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.523882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19580, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu2.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.547300Z",
     "iopub.status.busy": "2024-05-08T06:42:31.545136Z",
     "iopub.status.idle": "2024-05-08T06:42:31.556641Z",
     "shell.execute_reply": "2024-05-08T06:42:31.556195Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.547245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 102], x=[23, 1], y=[1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:42:31.557650Z",
     "iopub.status.busy": "2024-05-08T06:42:31.557356Z",
     "iopub.status.idle": "2024-05-08T06:42:31.578547Z",
     "shell.execute_reply": "2024-05-08T06:42:31.569864Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.557636Z"
    }
   },
   "outputs": [],
   "source": [
    "# just to stop before training the model for sake of sanity check \n",
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:44:15.815563Z",
     "iopub.status.busy": "2024-05-08T06:44:15.814427Z",
     "iopub.status.idle": "2024-05-08T06:44:15.831245Z",
     "shell.execute_reply": "2024-05-08T06:44:15.830293Z",
     "shell.execute_reply.started": "2024-05-08T06:44:15.815523Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# tu_dataset.shuffle()\n",
    "# train_dataset = tu_dataset[:500]\n",
    "# test_dataset = tu_dataset[500:]\n",
    "torch.manual_seed(42)\n",
    "tu2_shuffle=tu2.shuffle()\n",
    "train_dataset = tu2_shuffle[:500]\n",
    "test_dataset = tu2_shuffle[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a data loader, we encounter a problem with batching $N$ graphs. Each graph in the batch can have a different number of nodes and edges, and hence we would require a lot of padding to obtain a single tensor. Torch geometric uses a different, more efficient approach: we can view the $N$ graphs in a batch as a single large graph with concatenated node and edge list. As there is no edge between the $N$ graphs, running GNN layers on the large graph gives us the same output as running the GNN on each graph separately. Visually, this batching strategy is visualized below (figure credit - PyTorch Geometric team, [tutorial here](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=2owRWKcuoALo)).\n",
    "\n",
    "<center width=\"100%\"><img src=\"torch_geometric_stacking_graphs.png\" width=\"600px\"></center>\n",
    "\n",
    "The adjacency matrix is zero for any nodes that come from two different graphs, and otherwise according to the adjacency matrix of the individual graph. Luckily, this strategy is already implemented in torch geometric, and hence we can use the corresponding data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:47:33.663053Z",
     "iopub.status.busy": "2024-05-08T06:47:33.661531Z",
     "iopub.status.idle": "2024-05-08T06:47:33.673305Z",
     "shell.execute_reply": "2024-05-08T06:47:33.672631Z",
     "shell.execute_reply.started": "2024-05-08T06:47:33.662991Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=260, shuffle=True)\n",
    "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=15) # Additional loader if you want to change to a larger dataset\n",
    "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a batch below to see the batching in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:44:38.813787Z",
     "iopub.status.busy": "2024-05-08T06:44:38.812965Z",
     "iopub.status.idle": "2024-05-08T06:44:38.835731Z",
     "shell.execute_reply": "2024-05-08T06:44:38.835237Z",
     "shell.execute_reply.started": "2024-05-08T06:44:38.813700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 2192], x=[569, 1], y=[15], batch=[569], ptr=[16])\n",
      "Labels: tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(graph_test_loader))\n",
    "print(\"Batch:\", batch)\n",
    "print(\"Labels:\", batch.y[:10])\n",
    "print(\"Batch indices:\", batch.batch[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 38 graphs stacked together for the test dataset. The batch indices, stored in `batch`, show that the first 12 nodes belong to the first graph, the next 22 to the second graph, and so on. These indices are important for performing the final prediction. To perform a prediction over a whole graph, we usually perform a pooling operation over all nodes after running the GNN model. In this case, we will use the average pooling. Hence, we need to know which nodes should be included in which average pool. Using this pooling, we can already create our graph network below. Specifically, we re-use our class `GNNModel` from before, and simply add an average pool and single linear layer for the graph prediction task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's perform the training and testing. Feel free to experiment with different GNN layers, hyperparameters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_classifier(model_name,train_loader=graph_train_loader,test_loader=graph_test_loader, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         # callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"cpu\",# if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=500,\n",
    "                         enable_progress_bar=False)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        # print(\"Found pretrained model, loading...\")\n",
    "        # model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "        pl.seed_everything(42)\n",
    "        model = GraphLevelGNN(c_in=tu2.num_node_features, \n",
    "                              c_out=1 if tu2.num_classes==2 else tu2.num_classes, \n",
    "                              **model_kwargs)\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "        # model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = GraphLevelGNN(c_in=tu2.num_node_features, \n",
    "                              c_out=1 if tu2.num_classes==2 else tu2.num_classes, \n",
    "                              **model_kwargs)\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "        # model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    # test_pred = trainer.predict(model, graph_test_loader, return_predictions=True)\n",
    "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']\n",
    "              # ,\"pred_y\": test_pred\n",
    "            } \n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pytorch_lightning.trainer in pytorch_lightning:\n",
      "\n",
      "NAME\n",
      "    pytorch_lightning.trainer\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright The Lightning AI team.\n",
      "    #\n",
      "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "    # you may not use this file except in compliance with the License.\n",
      "    # You may obtain a copy of the License at\n",
      "    #\n",
      "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    call\n",
      "    configuration_validator\n",
      "    connectors (package)\n",
      "    setup\n",
      "    states\n",
      "    trainer\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        pytorch_lightning.trainer.trainer.Trainer\n",
      "    \n",
      "    class Trainer(builtins.object)\n",
      "     |  Trainer(*, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator] = 'auto', strategy: Union[str, pytorch_lightning.strategies.strategy.Strategy] = 'auto', devices: Union[List[int], str, int] = 'auto', num_nodes: int = 1, precision: Union[Literal[64, 32, 16], Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'], Literal['64', '32', '16', 'bf16'], NoneType] = None, logger: Union[pytorch_lightning.loggers.logger.Logger, Iterable[pytorch_lightning.loggers.logger.Logger], bool, NoneType] = None, callbacks: Union[List[pytorch_lightning.callbacks.callback.Callback], pytorch_lightning.callbacks.callback.Callback, NoneType] = None, fast_dev_run: Union[int, bool] = False, max_epochs: Optional[int] = None, min_epochs: Optional[int] = None, max_steps: int = -1, min_steps: Optional[int] = None, max_time: Union[str, datetime.timedelta, Dict[str, int], NoneType] = None, limit_train_batches: Union[int, float, NoneType] = None, limit_val_batches: Union[int, float, NoneType] = None, limit_test_batches: Union[int, float, NoneType] = None, limit_predict_batches: Union[int, float, NoneType] = None, overfit_batches: Union[int, float] = 0.0, val_check_interval: Union[int, float, NoneType] = None, check_val_every_n_epoch: Optional[int] = 1, num_sanity_val_steps: Optional[int] = None, log_every_n_steps: Optional[int] = None, enable_checkpointing: Optional[bool] = None, enable_progress_bar: Optional[bool] = None, enable_model_summary: Optional[bool] = None, accumulate_grad_batches: int = 1, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None, deterministic: Union[bool, Literal['warn'], NoneType] = None, benchmark: Optional[bool] = None, inference_mode: bool = True, use_distributed_sampler: bool = True, profiler: Union[pytorch_lightning.profilers.profiler.Profiler, str, NoneType] = None, detect_anomaly: bool = False, barebones: bool = False, plugins: Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync, List[Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync]], NoneType] = None, sync_batchnorm: bool = False, reload_dataloaders_every_n_epochs: int = 0, default_root_dir: Union[str, pathlib.Path, NoneType] = None) -> None\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator] = 'auto', strategy: Union[str, pytorch_lightning.strategies.strategy.Strategy] = 'auto', devices: Union[List[int], str, int] = 'auto', num_nodes: int = 1, precision: Union[Literal[64, 32, 16], Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'], Literal['64', '32', '16', 'bf16'], NoneType] = None, logger: Union[pytorch_lightning.loggers.logger.Logger, Iterable[pytorch_lightning.loggers.logger.Logger], bool, NoneType] = None, callbacks: Union[List[pytorch_lightning.callbacks.callback.Callback], pytorch_lightning.callbacks.callback.Callback, NoneType] = None, fast_dev_run: Union[int, bool] = False, max_epochs: Optional[int] = None, min_epochs: Optional[int] = None, max_steps: int = -1, min_steps: Optional[int] = None, max_time: Union[str, datetime.timedelta, Dict[str, int], NoneType] = None, limit_train_batches: Union[int, float, NoneType] = None, limit_val_batches: Union[int, float, NoneType] = None, limit_test_batches: Union[int, float, NoneType] = None, limit_predict_batches: Union[int, float, NoneType] = None, overfit_batches: Union[int, float] = 0.0, val_check_interval: Union[int, float, NoneType] = None, check_val_every_n_epoch: Optional[int] = 1, num_sanity_val_steps: Optional[int] = None, log_every_n_steps: Optional[int] = None, enable_checkpointing: Optional[bool] = None, enable_progress_bar: Optional[bool] = None, enable_model_summary: Optional[bool] = None, accumulate_grad_batches: int = 1, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None, deterministic: Union[bool, Literal['warn'], NoneType] = None, benchmark: Optional[bool] = None, inference_mode: bool = True, use_distributed_sampler: bool = True, profiler: Union[pytorch_lightning.profilers.profiler.Profiler, str, NoneType] = None, detect_anomaly: bool = False, barebones: bool = False, plugins: Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync, List[Union[pytorch_lightning.plugins.precision.precision.Precision, lightning_fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning_fabric.plugins.io.checkpoint_io.CheckpointIO, pytorch_lightning.plugins.layer_sync.LayerSync]], NoneType] = None, sync_batchnorm: bool = False, reload_dataloaders_every_n_epochs: int = 0, default_root_dir: Union[str, pathlib.Path, NoneType] = None) -> None\n",
      "     |      Customize every aspect of training via flags.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"mps\", \"auto\")\n",
      "     |              as well as custom accelerator instances.\n",
      "     |      \n",
      "     |          strategy: Supports different training strategies with aliases as well custom strategies.\n",
      "     |              Default: ``\"auto\"``.\n",
      "     |      \n",
      "     |          devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\n",
      "     |              (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\n",
      "     |              automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\n",
      "     |      \n",
      "     |          num_nodes: Number of GPU nodes for distributed training.\n",
      "     |              Default: ``1``.\n",
      "     |      \n",
      "     |          precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n",
      "     |              16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n",
      "     |              Can be used on CPU, GPU, TPUs, HPUs or IPUs.\n",
      "     |              Default: ``'32-true'``.\n",
      "     |      \n",
      "     |          logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
      "     |              the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\n",
      "     |              ``False`` will disable logging. If multiple loggers are provided, local files\n",
      "     |              (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\n",
      "     |              Default: ``True``.\n",
      "     |      \n",
      "     |          callbacks: Add a callback or list of callbacks.\n",
      "     |              Default: ``None``.\n",
      "     |      \n",
      "     |          fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      "     |              of train, val and test to find any bugs (ie: a sort of unit test).\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |          max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      "     |              If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
      "     |              To enable infinite training, set ``max_epochs = -1``.\n",
      "     |      \n",
      "     |          min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      "     |      \n",
      "     |          max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
      "     |              and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
      "     |              ``max_epochs`` to ``-1``.\n",
      "     |      \n",
      "     |          min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
      "     |      \n",
      "     |          max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
      "     |              The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
      "     |              :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
      "     |              :class:`datetime.timedelta`.\n",
      "     |      \n",
      "     |          limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
      "     |              Default: ``1.0``.\n",
      "     |      \n",
      "     |          limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
      "     |              Default: ``1.0``.\n",
      "     |      \n",
      "     |          limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
      "     |              Default: ``1.0``.\n",
      "     |      \n",
      "     |          limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
      "     |              Default: ``1.0``.\n",
      "     |      \n",
      "     |          overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\n",
      "     |              Default: ``0.0``.\n",
      "     |      \n",
      "     |          val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
      "     |              after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
      "     |              batches. An ``int`` value can only be higher than the number of training batches when\n",
      "     |              ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\n",
      "     |              across epochs or during iteration-based training.\n",
      "     |              Default: ``1.0``.\n",
      "     |      \n",
      "     |          check_val_every_n_epoch: Perform a validation loop every after every `N` training epochs. If ``None``,\n",
      "     |              validation will be done solely based on the number of training batches, requiring ``val_check_interval``\n",
      "     |              to be an integer value.\n",
      "     |              Default: ``1``.\n",
      "     |      \n",
      "     |          num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      "     |              Set it to `-1` to run all batches in all validation dataloaders.\n",
      "     |              Default: ``2``.\n",
      "     |      \n",
      "     |          log_every_n_steps: How often to log within steps.\n",
      "     |              Default: ``50``.\n",
      "     |      \n",
      "     |          enable_checkpointing: If ``True``, enable checkpointing.\n",
      "     |              It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n",
      "     |              Default: ``True``.\n",
      "     |      \n",
      "     |          enable_progress_bar: Whether to enable to progress bar by default.\n",
      "     |              Default: ``True``.\n",
      "     |      \n",
      "     |          enable_model_summary: Whether to enable model summarization by default.\n",
      "     |              Default: ``True``.\n",
      "     |      \n",
      "     |          accumulate_grad_batches: Accumulates gradients over k batches before stepping the optimizer.\n",
      "     |              Default: 1.\n",
      "     |      \n",
      "     |          gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
      "     |              gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
      "     |              Default: ``None``.\n",
      "     |      \n",
      "     |          gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
      "     |              to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
      "     |              be set to ``\"norm\"``.\n",
      "     |      \n",
      "     |          deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
      "     |              Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\n",
      "     |              that don't support deterministic mode. If not set, defaults to ``False``. Default: ``None``.\n",
      "     |      \n",
      "     |          benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\n",
      "     |              The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\n",
      "     |              (``False`` if not manually set). If :paramref:`~pytorch_lightning.trainer.trainer.Trainer.deterministic`\n",
      "     |              is set to ``True``, this will default to ``False``. Override to manually set a different value.\n",
      "     |              Default: ``None``.\n",
      "     |      \n",
      "     |          inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\n",
      "     |              evaluation (``validate``/``test``/``predict``).\n",
      "     |      \n",
      "     |          use_distributed_sampler: Whether to wrap the DataLoader's sampler with\n",
      "     |              :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for\n",
      "     |              strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and\n",
      "     |              ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass\n",
      "     |              ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed\n",
      "     |              sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,\n",
      "     |              we don't do this automatically.\n",
      "     |      \n",
      "     |          profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
      "     |              Default: ``None``.\n",
      "     |      \n",
      "     |          detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |          barebones: Whether to run in \"barebones mode\", where all features that may impact raw speed are\n",
      "     |              disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training\n",
      "     |              runs. The following features are deactivated:\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_checkpointing`,\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.logger`,\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_progress_bar`,\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.log_every_n_steps`,\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_model_summary`,\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.num_sanity_val_steps`,\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.fast_dev_run`,\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.detect_anomaly`,\n",
      "     |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.profiler`,\n",
      "     |              :meth:`~pytorch_lightning.core.LightningModule.log`,\n",
      "     |              :meth:`~pytorch_lightning.core.LightningModule.log_dict`.\n",
      "     |          plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      "     |              Default: ``None``.\n",
      "     |      \n",
      "     |          sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |          reload_dataloaders_every_n_epochs: Set to a positive integer to reload dataloaders every n epochs.\n",
      "     |              Default: ``0``.\n",
      "     |      \n",
      "     |          default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      "     |              Default: ``os.getcwd()``.\n",
      "     |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          TypeError:\n",
      "     |              If ``gradient_clip_val`` is not an int or float.\n",
      "     |      \n",
      "     |          MisconfigurationException:\n",
      "     |              If ``gradient_clip_algorithm`` is invalid.\n",
      "     |  \n",
      "     |  fit(self, model: 'pl.LightningModule', train_dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, val_dataloaders: Optional[Any] = None, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None) -> None\n",
      "     |      Runs the full optimization routine.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          model: Model to fit.\n",
      "     |      \n",
      "     |          train_dataloaders: An iterable or collection of iterables specifying training samples.\n",
      "     |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      "     |              the :class:`~pytorch_lightning.core.hooks.DataHooks.train_dataloader` hook.\n",
      "     |      \n",
      "     |          val_dataloaders: An iterable or collection of iterables specifying validation samples.\n",
      "     |      \n",
      "     |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      "     |              the :class:`~pytorch_lightning.core.hooks.DataHooks.train_dataloader` hook.\n",
      "     |      \n",
      "     |          ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of two special\n",
      "     |              keywords ``\"last\"`` and ``\"hpc\"``. If there is no checkpoint file at the path, an exception is raised.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          TypeError:\n",
      "     |              If ``model`` is not :class:`~pytorch_lightning.core.LightningModule` for torch version less than\n",
      "     |              2.0.0 and if ``model`` is not :class:`~pytorch_lightning.core.LightningModule` or\n",
      "     |              :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .\n",
      "     |      \n",
      "     |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      "     |  \n",
      "     |  init_module(self, empty_init: Optional[bool] = None) -> Generator\n",
      "     |      Tensors that you instantiate under this context manager will be created on the device right away and have\n",
      "     |      the right data type depending on the precision setting in the Trainer.\n",
      "     |      \n",
      "     |      The parameters and tensors get created on the device and with the right data type right away without wasting\n",
      "     |      memory being allocated unnecessarily. The automatic device placement under this context manager is only\n",
      "     |      supported with PyTorch 2.0 and newer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          empty_init: Whether to initialize the model with empty weights (uninitialized memory).\n",
      "     |              If ``None``, the strategy will decide. Some strategies may not support all options.\n",
      "     |              Set this to ``True`` if you are loading a checkpoint into a large model.\n",
      "     |  \n",
      "     |  predict(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None, return_predictions: Optional[bool] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None) -> Union[List[Any], List[List[Any]], NoneType]\n",
      "     |      Run inference on your data. This will call the model forward function to compute predictions. Useful to\n",
      "     |      perform distributed and batched predictions. Logging is disabled in the predict hooks.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          model: The model to predict with.\n",
      "     |      \n",
      "     |          dataloaders: An iterable or collection of iterables specifying predict samples.\n",
      "     |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      "     |              the :class:`~pytorch_lightning.core.hooks.DataHooks.predict_dataloader` hook.\n",
      "     |      \n",
      "     |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      "     |              the :class:`~pytorch_lightning.core.hooks.DataHooks.predict_dataloader` hook.\n",
      "     |      \n",
      "     |          return_predictions: Whether to return predictions.\n",
      "     |              ``True`` by default except when an accelerator that spawns processes is used (not supported).\n",
      "     |      \n",
      "     |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"`` or path to the checkpoint you wish to predict.\n",
      "     |              If ``None`` and the model instance was passed, use the current weights.\n",
      "     |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      "     |              if a checkpoint callback is configured.\n",
      "     |      \n",
      "     |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          TypeError:\n",
      "     |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      "     |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      "     |      \n",
      "     |          MisconfigurationException:\n",
      "     |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      "     |      \n",
      "     |          RuntimeError:\n",
      "     |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      "     |      \n",
      "     |      See :ref:`Lightning inference section<deploy/production_basic:Predict step with your LightningModule>` for more.\n",
      "     |  \n",
      "     |  print(self, *args: Any, **kwargs: Any) -> None\n",
      "     |      Print something only on the first process. If running on multiple machines, it will print from the first\n",
      "     |      process in each machine.\n",
      "     |      \n",
      "     |      Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\n",
      "     |  \n",
      "     |  save_checkpoint(self, filepath: Union[str, pathlib.Path], weights_only: bool = False, storage_options: Optional[Any] = None) -> None\n",
      "     |      Runs routine to create a checkpoint.\n",
      "     |      \n",
      "     |      This method needs to be called on all processes in case the selected strategy is handling distributed\n",
      "     |      checkpointing.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          filepath: Path where checkpoint is saved.\n",
      "     |          weights_only: If ``True``, will only save the model weights.\n",
      "     |          storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError:\n",
      "     |              If the model is not attached to the Trainer before calling this method.\n",
      "     |  \n",
      "     |  test(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None, verbose: bool = True, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None) -> List[Mapping[str, float]]\n",
      "     |      Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your\n",
      "     |      test set until you want to.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          model: The model to test.\n",
      "     |      \n",
      "     |          dataloaders: An iterable or collection of iterables specifying test samples.\n",
      "     |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      "     |              the :class:`~pytorch_lightning.core.hooks.DataHooks.test_dataloader` hook.\n",
      "     |      \n",
      "     |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"`` or path to the checkpoint you wish to test.\n",
      "     |              If ``None`` and the model instance was passed, use the current weights.\n",
      "     |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      "     |              if a checkpoint callback is configured.\n",
      "     |      \n",
      "     |          verbose: If True, prints the test results.\n",
      "     |      \n",
      "     |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      "     |              the :class:`~pytorch_lightning.core.hooks.DataHooks.test_dataloader` hook.\n",
      "     |      \n",
      "     |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks\n",
      "     |          like :meth:`~pytorch_lightning.LightningModule.test_step` etc.\n",
      "     |          The length of the list corresponds to the number of test dataloaders used.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          TypeError:\n",
      "     |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      "     |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      "     |      \n",
      "     |          MisconfigurationException:\n",
      "     |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      "     |      \n",
      "     |          RuntimeError:\n",
      "     |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      "     |  \n",
      "     |  validate(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None, verbose: bool = True, datamodule: Optional[pytorch_lightning.core.datamodule.LightningDataModule] = None) -> List[Mapping[str, float]]\n",
      "     |      Perform one evaluation epoch over the validation set.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          model: The model to validate.\n",
      "     |      \n",
      "     |          dataloaders: An iterable or collection of iterables specifying validation samples.\n",
      "     |              Alternatively, a :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      "     |              the :class:`~pytorch_lightning.core.hooks.DataHooks.val_dataloader` hook.\n",
      "     |      \n",
      "     |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"`` or path to the checkpoint you wish to validate.\n",
      "     |              If ``None`` and the model instance was passed, use the current weights.\n",
      "     |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      "     |              if a checkpoint callback is configured.\n",
      "     |      \n",
      "     |          verbose: If True, prints the validation results.\n",
      "     |      \n",
      "     |          datamodule: A :class:`~pytorch_lightning.core.datamodule.LightningDataModule` that defines\n",
      "     |              the :class:`~pytorch_lightning.core.hooks.DataHooks.val_dataloader` hook.\n",
      "     |      \n",
      "     |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks\n",
      "     |          like :meth:`~pytorch_lightning.LightningModule.validation_step` etc.\n",
      "     |          The length of the list corresponds to the number of validation dataloaders used.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          TypeError:\n",
      "     |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      "     |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      "     |      \n",
      "     |          MisconfigurationException:\n",
      "     |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      "     |      \n",
      "     |          RuntimeError:\n",
      "     |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  accelerator\n",
      "     |  \n",
      "     |  callback_metrics\n",
      "     |      The metrics available to callbacks.\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          def training_step(self, batch, batch_idx):\n",
      "     |              self.log(\"a_val\", 2.0)\n",
      "     |      \n",
      "     |      \n",
      "     |          callback_metrics = trainer.callback_metrics\n",
      "     |          assert callback_metrics[\"a_val\"] == 2.0\n",
      "     |  \n",
      "     |  checkpoint_callback\n",
      "     |      The first :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` callback in the\n",
      "     |      Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      "     |  \n",
      "     |  checkpoint_callbacks\n",
      "     |      A list of all instances of :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` found in\n",
      "     |      the Trainer.callbacks list.\n",
      "     |  \n",
      "     |  current_epoch\n",
      "     |      The current epoch, updated after the epoch end hooks are run.\n",
      "     |  \n",
      "     |  default_root_dir\n",
      "     |      The default location to save artifacts of loggers, checkpoints etc.\n",
      "     |      \n",
      "     |      It is used as a fallback if logger or checkpoint callback do not define specific save paths.\n",
      "     |  \n",
      "     |  device_ids\n",
      "     |      List of device indexes per node.\n",
      "     |  \n",
      "     |  distributed_sampler_kwargs\n",
      "     |  \n",
      "     |  early_stopping_callback\n",
      "     |      The first :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping` callback in the\n",
      "     |      Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      "     |  \n",
      "     |  early_stopping_callbacks\n",
      "     |      A list of all instances of :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping` found in the\n",
      "     |      Trainer.callbacks list.\n",
      "     |  \n",
      "     |  enable_validation\n",
      "     |      Check if we should run validation during training.\n",
      "     |  \n",
      "     |  estimated_stepping_batches\n",
      "     |      The estimated number of batches that will ``optimizer.step()`` during training.\n",
      "     |      \n",
      "     |      This accounts for gradient accumulation and the current trainer configuration. This might sets up your training\n",
      "     |      dataloader if hadn't been set up already.\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          def configure_optimizers(self):\n",
      "     |              optimizer = ...\n",
      "     |              stepping_batches = self.trainer.estimated_stepping_batches\n",
      "     |              scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\n",
      "     |              return [optimizer], [scheduler]\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          MisconfigurationException:\n",
      "     |              If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\n",
      "     |              at different epochs.\n",
      "     |  \n",
      "     |  evaluating\n",
      "     |  \n",
      "     |  global_rank\n",
      "     |  \n",
      "     |  global_step\n",
      "     |      The number of optimizer steps taken (does not reset each epoch).\n",
      "     |      \n",
      "     |      This includes multiple optimizers (if enabled).\n",
      "     |  \n",
      "     |  interrupted\n",
      "     |  \n",
      "     |  is_global_zero\n",
      "     |      Whether this process is the global zero in multi-node training.\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          def training_step(self, batch, batch_idx):\n",
      "     |              if self.trainer.is_global_zero:\n",
      "     |                  print(\"in node 0, accelerator 0\")\n",
      "     |  \n",
      "     |  is_last_batch\n",
      "     |      Whether trainer is executing the last batch.\n",
      "     |  \n",
      "     |  lightning_module\n",
      "     |  \n",
      "     |  local_rank\n",
      "     |  \n",
      "     |  log_dir\n",
      "     |      The directory for the current experiment. Use this to save images to, etc...\n",
      "     |      \n",
      "     |      .. note:: You must call this on all processes. Failing to do so will cause your program to stall forever.\n",
      "     |      \n",
      "     |       .. code-block:: python\n",
      "     |      \n",
      "     |           def training_step(self, batch, batch_idx):\n",
      "     |               img = ...\n",
      "     |               save_img(img, self.trainer.log_dir)\n",
      "     |  \n",
      "     |  logged_metrics\n",
      "     |      The metrics sent to the loggers.\n",
      "     |      \n",
      "     |      This includes metrics logged via :meth:`~pytorch_lightning.core.LightningModule.log` with the\n",
      "     |      :paramref:`~pytorch_lightning.core.LightningModule.log.logger` argument set.\n",
      "     |  \n",
      "     |  lr_scheduler_configs\n",
      "     |  \n",
      "     |  max_epochs\n",
      "     |  \n",
      "     |  max_steps\n",
      "     |  \n",
      "     |  min_epochs\n",
      "     |  \n",
      "     |  min_steps\n",
      "     |  \n",
      "     |  model\n",
      "     |      The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\n",
      "     |      \n",
      "     |      To access the pure LightningModule, use\n",
      "     |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.lightning_module` instead.\n",
      "     |  \n",
      "     |  node_rank\n",
      "     |  \n",
      "     |  num_devices\n",
      "     |      Number of devices the trainer uses per node.\n",
      "     |  \n",
      "     |  num_nodes\n",
      "     |  \n",
      "     |  num_predict_batches\n",
      "     |      The number of prediction batches that will be used during ``trainer.predict()``.\n",
      "     |  \n",
      "     |  num_sanity_val_batches\n",
      "     |      The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.\n",
      "     |  \n",
      "     |  num_test_batches\n",
      "     |      The number of test batches that will be used during ``trainer.test()``.\n",
      "     |  \n",
      "     |  num_training_batches\n",
      "     |      The number of training batches that will be used during ``trainer.fit()``.\n",
      "     |  \n",
      "     |  num_val_batches\n",
      "     |      The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.\n",
      "     |  \n",
      "     |  precision\n",
      "     |  \n",
      "     |  precision_plugin\n",
      "     |  \n",
      "     |  predict_dataloaders\n",
      "     |      The prediction dataloader(s) used during ``trainer.predict()``.\n",
      "     |  \n",
      "     |  progress_bar_callback\n",
      "     |      An instance of :class:`~pytorch_lightning.callbacks.progress.progress_bar.ProgressBar` found in the\n",
      "     |      Trainer.callbacks list, or ``None`` if one doesn't exist.\n",
      "     |  \n",
      "     |  progress_bar_metrics\n",
      "     |      The metrics sent to the progress bar.\n",
      "     |      \n",
      "     |      This includes metrics logged via :meth:`~pytorch_lightning.core.LightningModule.log` with the\n",
      "     |      :paramref:`~pytorch_lightning.core.LightningModule.log.prog_bar` argument set.\n",
      "     |  \n",
      "     |  received_sigterm\n",
      "     |      Whether a ``signal.SIGTERM`` signal was received.\n",
      "     |      \n",
      "     |      For example, this can be checked to exit gracefully.\n",
      "     |  \n",
      "     |  scaler\n",
      "     |  \n",
      "     |  strategy\n",
      "     |  \n",
      "     |  test_dataloaders\n",
      "     |      The test dataloader(s) used during ``trainer.test()``.\n",
      "     |  \n",
      "     |  train_dataloader\n",
      "     |      The training dataloader(s) used during ``trainer.fit()``.\n",
      "     |  \n",
      "     |  val_dataloaders\n",
      "     |      The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.\n",
      "     |  \n",
      "     |  world_size\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ckpt_path\n",
      "     |      Set to the path/URL of a checkpoint loaded via :meth:`~pytorch_lightning.trainer.trainer.Trainer.fit`,\n",
      "     |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.validate`,\n",
      "     |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.test`, or\n",
      "     |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.predict`.\n",
      "     |      \n",
      "     |      ``None`` otherwise.\n",
      "     |  \n",
      "     |  logger\n",
      "     |      The first :class:`~pytorch_lightning.loggers.logger.Logger` being used.\n",
      "     |  \n",
      "     |  loggers\n",
      "     |      The list of :class:`~pytorch_lightning.loggers.logger.Logger` used.\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          for logger in trainer.loggers:\n",
      "     |              logger.log_metrics({\"foo\": 1.0})\n",
      "     |  \n",
      "     |  optimizers\n",
      "     |  \n",
      "     |  predicting\n",
      "     |  \n",
      "     |  sanity_checking\n",
      "     |      Whether sanity checking is running.\n",
      "     |      \n",
      "     |      Useful to disable some hooks, logging or callbacks during the sanity checking.\n",
      "     |  \n",
      "     |  testing\n",
      "     |  \n",
      "     |  training\n",
      "     |  \n",
      "     |  validating\n",
      "\n",
      "FUNCTIONS\n",
      "    seed_everything(seed: Optional[int] = None, workers: bool = False) -> int\n",
      "        Function that sets the seed for pseudo-random number generators in: torch, numpy, and Python's random module.\n",
      "        In addition, sets the following environment variables:\n",
      "        \n",
      "        - ``PL_GLOBAL_SEED``: will be passed to spawned subprocesses (e.g. ddp_spawn backend).\n",
      "        - ``PL_SEED_WORKERS``: (optional) is set to 1 if ``workers=True``.\n",
      "        \n",
      "        Args:\n",
      "            seed: the integer value seed for global random state in Lightning.\n",
      "                If ``None``, it will read the seed from ``PL_GLOBAL_SEED`` env variable. If ``None`` and the\n",
      "                ``PL_GLOBAL_SEED`` env variable is not set, then the seed defaults to 0.\n",
      "            workers: if set to ``True``, will properly configure all dataloaders passed to the\n",
      "                Trainer with a ``worker_init_fn``. If the user already provides such a function\n",
      "                for their dataloaders, setting this argument will have no influence. See also:\n",
      "                :func:`~lightning_fabric.utilities.seed.pl_worker_init_function`.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Trainer', 'seed_everything']\n",
      "\n",
      "FILE\n",
      "    /Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pytorch_lightning/trainer/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pl.trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.602429Z",
     "iopub.status.idle": "2024-05-08T06:42:31.602883Z",
     "shell.execute_reply": "2024-05-08T06:42:31.602555Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.602515Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Seed set to 42\n",
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | model       | GraphGNNModel     | 1.1 K \n",
      "1 | loss_module | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n",
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "model, result = train_graph_classifier(model_name=\"GraphConv\", \n",
    "                                       c_hidden=16, \n",
    "                                       layer_name=\"GraphConv\", \n",
    "                                       num_layers=3, \n",
    "                                       dp_rate_linear=0,\n",
    "                                       dp_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.606678Z",
     "iopub.status.idle": "2024-05-08T06:42:31.606865Z",
     "shell.execute_reply": "2024-05-08T06:42:31.606755Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.606748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train performance: 100.00%\n",
      "Test performance:  100.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
    "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T16:53:18.079506Z",
     "iopub.status.busy": "2024-02-25T16:53:18.079324Z",
     "iopub.status.idle": "2024-02-25T16:53:18.397691Z",
     "shell.execute_reply": "2024-02-25T16:53:18.394476Z",
     "shell.execute_reply.started": "2024-02-25T16:53:18.079494Z"
    }
   },
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.613898Z",
     "iopub.status.idle": "2024-05-08T06:42:31.614092Z",
     "shell.execute_reply": "2024-05-08T06:42:31.613994Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.613986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[43471, 3], edge_index=[2, 162088], y=[1113])\n",
      "Length: 1113\n",
      "Average label: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "proteins_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"PROTEINS\")\n",
    "print(\"Data object:\", proteins_dataset.data)\n",
    "print(\"Length:\", len(proteins_dataset))\n",
    "print(f\"Average label: {proteins_dataset.data.y.float().mean().item():4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.614674Z",
     "iopub.status.idle": "2024-05-08T06:42:31.614870Z",
     "shell.execute_reply": "2024-05-08T06:42:31.614738Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.614732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[43471, 3], edge_index=[2, 162088], y=[1113])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins_dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a different dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.615691Z",
     "iopub.status.idle": "2024-05-08T06:42:31.616049Z",
     "shell.execute_reply": "2024-05-08T06:42:31.615949Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.615935Z"
    }
   },
   "outputs": [],
   "source": [
    "tu3=proteins_dataset.copy()\n",
    "tu3.data.x=tu3.data.x[:,:1]\n",
    "# tu2.data.x=torch.ones(tu2.x.shape)\n",
    "tu3.data.x=torch.randn(tu3.x.shape)\n",
    "for i in range(len(tu3.y)): \n",
    "    tu3.y[i]=max(torch.bincount(tu3[i].edge_index[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.616686Z",
     "iopub.status.idle": "2024-05-08T06:42:31.616840Z",
     "shell.execute_reply": "2024-05-08T06:42:31.616767Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.616760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 5, 5, 7, 5, 7, 7, 8, 5, 5, 4, 8, 5, 6, 6, 5, 5, 7, 7, 5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu3.y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.617266Z",
     "iopub.status.idle": "2024-05-08T06:42:31.617437Z",
     "shell.execute_reply": "2024-05-08T06:42:31.617337Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.617330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(-120.8686, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  tensor(0.),\n",
       "  tensor([1.])),\n",
       " (tensor(151.6385, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  tensor(0.),\n",
       "  tensor([0.])),\n",
       " (tensor(134.9324, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  tensor(0.),\n",
       "  tensor([0.])),\n",
       " (tensor(-53.9077, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  tensor(0.),\n",
       "  tensor([1.])),\n",
       " (tensor(123.2618, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  tensor(0.),\n",
       "  tensor([0.])),\n",
       " (tensor(-64.3469, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  tensor(0.),\n",
       "  tensor([1.])),\n",
       " (tensor(-60.1594, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  tensor(0.),\n",
       "  tensor([1.])),\n",
       " (tensor(-167.3324, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  tensor(0.),\n",
       "  tensor([1.])))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tu3[0]),model(tu3[1]),model(tu3[2]),model(tu3[3]),model(tu3[4]),model(tu3[5]),model(tu3[6]),model(tu3[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.618619Z",
     "iopub.status.idle": "2024-05-08T06:42:31.619896Z",
     "shell.execute_reply": "2024-05-08T06:42:31.619414Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.619256Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu3.data.y=(tu3.y>thres).long()\n",
    "tu3.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.621775Z",
     "iopub.status.idle": "2024-05-08T06:42:31.622283Z",
     "shell.execute_reply": "2024-05-08T06:42:31.622145Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.622135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu3.y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.623118Z",
     "iopub.status.idle": "2024-05-08T06:42:31.623676Z",
     "shell.execute_reply": "2024-05-08T06:42:31.623412Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.623357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(234)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu3.data.y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.625477Z",
     "iopub.status.idle": "2024-05-08T06:42:31.625878Z",
     "shell.execute_reply": "2024-05-08T06:42:31.625672Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.625657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beatrixwen/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "proteins_test_loader = geom_data.DataLoader(tu3[:100], batch_size=1)\n",
    "# proteins_result = trainer.test(model, proteins_test_loader, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.633353Z",
     "iopub.status.idle": "2024-05-08T06:42:31.633840Z",
     "shell.execute_reply": "2024-05-08T06:42:31.633653Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.633641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphLevelGNN(\n",
       "  (model): GraphGNNModel(\n",
       "    (GNN): GNNModel(\n",
       "      (layers): ModuleList(\n",
       "        (0): GraphConv(1, 16)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): GraphConv(16, 16)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): GraphConv(16, 16)\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_module): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.630162Z",
     "iopub.status.idle": "2024-05-08T06:42:31.630688Z",
     "shell.execute_reply": "2024-05-08T06:42:31.630405Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.630387Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Proposed Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.648587Z",
     "iopub.status.idle": "2024-05-08T06:42:31.649686Z",
     "shell.execute_reply": "2024-05-08T06:42:31.649329Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.649297Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_test_1 = tu2[0]\n",
    "graph_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.651427Z",
     "iopub.status.idle": "2024-05-08T06:42:31.651593Z",
     "shell.execute_reply": "2024-05-08T06:42:31.651518Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.651509Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.652121Z",
     "iopub.status.idle": "2024-05-08T06:42:31.652395Z",
     "shell.execute_reply": "2024-05-08T06:42:31.652287Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.652273Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.653105Z",
     "iopub.status.idle": "2024-05-08T06:42:31.653303Z",
     "shell.execute_reply": "2024-05-08T06:42:31.653177Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.653170Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.654163Z",
     "iopub.status.idle": "2024-05-08T06:42:31.654315Z",
     "shell.execute_reply": "2024-05-08T06:42:31.654246Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.654239Z"
    }
   },
   "outputs": [],
   "source": [
    "def excl_low_value_nodes(imp_nodes,excl_round=1,node_threshold=None): \n",
    "    for i in range(excl_round): \n",
    "        if min(imp_nodes.values())==max(imp_nodes.values()): \n",
    "            return imp_nodes\n",
    "        else: \n",
    "            imp_nodes = {key:val for key, val in imp_nodes.items() if val != min(imp_nodes.values()) }\n",
    "            print (i) \n",
    "            if node_threshold!=None: \n",
    "                if len(list(imp_nodes.keys()))<=node_threshold: \n",
    "                    return imp_nodes \n",
    "        # return imp_nodes \n",
    "\n",
    "    \n",
    "                         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.654898Z",
     "iopub.status.idle": "2024-05-08T06:42:31.655087Z",
     "shell.execute_reply": "2024-05-08T06:42:31.654993Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.654986Z"
    }
   },
   "outputs": [],
   "source": [
    "def shapley_dict_count(shapley_dict_lst,excl_min=0): \n",
    "    # if draw_graph==True: \n",
    "    #     draw_with_color(graph)\n",
    "    # i = shapley_dict[\"rank\"+str(level)].copy()\n",
    "    i = shapley_dict_lst.copy() \n",
    "    d = {x:i.count(x) for x in i}\n",
    "    sorted_dict = dict(sorted(d.items(), key=operator.itemgetter(1)))\n",
    "    if excl_min==0: \n",
    "        return sorted_dict\n",
    "    elif excl_min>=1: \n",
    "        if min(d.values())==max(d.values()): \n",
    "            imp_nodes=sorted_dict\n",
    "        else: \n",
    "            imp_nodes = {key:val for key, val in sorted_dict.items() if val != min(d.values()) }\n",
    "        if excl_min==2: \n",
    "            imp_nodes = {key:val for key, val in imp_nodes.items() if val != min(imp_nodes.values()) }\n",
    "        return imp_nodes \n",
    "    # graph_i=new_subgraph(graph,include_node_lst=list(imp_nodes.keys()))\n",
    "    # if draw_subgraph==True: \n",
    "    #     draw_with_color(graph_i)\n",
    "    # return graph_i,imp_nodes\n",
    "    \n",
    "                         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.657172Z",
     "iopub.status.idle": "2024-05-08T06:42:31.658782Z",
     "shell.execute_reply": "2024-05-08T06:42:31.658346Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.658293Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_subgraph(graph,include_node_lst=None,exclude_node_lst=None,add_node=False,add_sudo_nodes=False,node_lst=None, draw_subgraph=False): \n",
    "    if add_node!=False: \n",
    "        n=graph.x.shape[0]\n",
    "        num_edges_to_add=len(add_node)\n",
    "        # node_lst=[i for i in range(graph.x.shape[0])]\n",
    "        edge_index_new_0=torch.cat((graph.edge_index[0],int(n)*torch.ones(num_edges_to_add, dtype=torch.int32),torch.as_tensor(add_node, dtype=torch.int32)))\n",
    "        edge_index_new_1=torch.cat((graph.edge_index[1],torch.as_tensor(add_node, dtype=torch.int32),int(n)*torch.ones(num_edges_to_add, dtype=torch.int32)))\n",
    "        edge_index_new=torch.stack((edge_index_new_0,edge_index_new_1))\n",
    "        x_new=torch.cat((graph.x,torch.randn(1,1)))\n",
    "        graph = Data(x=x_new, edge_index=edge_index_new)\n",
    "        # graph= torch_geometric.utils.to_networkx(data_new)\n",
    "        # nx.draw(g_new,with_labels=True)\n",
    "        graph_i =graph \n",
    "\n",
    "        \n",
    "    if include_node_lst!=None: \n",
    "        subset = torch.zeros_like(graph.edge_index[0], dtype = bool)\n",
    "        subset[include_node_lst] = True\n",
    "        # print(include_node_lst,th_preserve(graph.x,[include_node_lst]))\n",
    "        graph_i = torch_geometric.data.Data(x=graph.x, #=th_preserve(graph.x,[include_node_lst]),\n",
    "                                            edge_index=torch_geometric.utils.subgraph(subset,graph.edge_index,relabel_nodes=False)[0],\n",
    "                                            y=graph.y)\n",
    "\n",
    "    if add_sudo_nodes==True: \n",
    "        n=graph.x.shape[0]\n",
    "        num_edges_to_add=len(node_lst)\n",
    "        # node_lst=[i for i in range(graph.x.shape[0])]\n",
    "        edge_index_new_0=torch.cat((graph.edge_index[0],int(n)*torch.ones(num_edges_to_add, dtype=torch.int32),torch.as_tensor(node_lst, dtype=torch.int32)))\n",
    "        edge_index_new_1=torch.cat((graph.edge_index[1],torch.as_tensor(node_lst, dtype=torch.int32),int(n)*torch.ones(num_edges_to_add, dtype=torch.int32)))\n",
    "        edge_index_new=torch.stack((edge_index_new_0,edge_index_new_1))\n",
    "        x_new=torch.cat((graph.x,torch.randn(1,1)))\n",
    "        graph = Data(x=x_new, edge_index=edge_index_new)\n",
    "        # graph= torch_geometric.utils.to_networkx(data_new)\n",
    "        # nx.draw(g_new,with_labels=True)\n",
    "        graph_i =graph \n",
    "        \n",
    "    if exclude_node_lst!=None: \n",
    "        subset = torch.ones_like(graph.edge_index[0], dtype = bool)\n",
    "        subset[exclude_node_lst] = False\n",
    "        graph_i = torch_geometric.data.Data(x=graph.x, #=th_delete(graph.x,[exclude_node_lst]),\n",
    "                                            edge_index=torch_geometric.utils.subgraph(subset,graph.edge_index,relabel_nodes=False)[0],\n",
    "                                            y=graph.y)\n",
    "    \n",
    "    if draw_subgraph==True: \n",
    "            draw_with_color(graph_i)\n",
    "    return graph_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.661380Z",
     "iopub.status.idle": "2024-05-08T06:42:31.662012Z",
     "shell.execute_reply": "2024-05-08T06:42:31.661858Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.661846Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.zeros(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.663175Z",
     "iopub.status.idle": "2024-05-08T06:42:31.664742Z",
     "shell.execute_reply": "2024-05-08T06:42:31.664022Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.664002Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_shapley(graph,level=1,shapley_storage=None,skip_to_round=1,previous_shapley=None): \n",
    "    graph_label=model(graph)[2]\n",
    "    node_num=graph.x.size()[0]\n",
    "    n=node_num\n",
    "    if shapley_storage==None: \n",
    "        shapley_storage = torch.zeros((n,n,n))\n",
    "    else: \n",
    "        shapley_storage=shapley_storage\n",
    "    shapley_dict={}\n",
    "    round=skip_to_round\n",
    "    rank=1\n",
    "\n",
    "    if round==1:     \n",
    "        shapley_dict[\"rank\"+str(rank)]=[]\n",
    "        for i in range(node_num):\n",
    "            # global round\n",
    "            # subset = torch.ones_like(graph.edge_index[0], dtype = bool)\n",
    "            # subset[[i]] = False\n",
    "            # graph_i = torch_geometric.data.Data(x=graph.x, #=th_delete(graph.x,[i]),\n",
    "            #                                     edge_index=torch_geometric.utils.subgraph(subset,graph.edge_index,relabel_nodes=True)[0],\n",
    "            #                                     y=graph.y)\n",
    "            graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i],add_sudo_nodes=False)  \n",
    "            shapley_i=graph_label-model(graph_i)[2] \n",
    "            # print(shapley_i)\n",
    "            if shapley_i!=0: \n",
    "                shapley_storage[i,0,0]=1\n",
    "                shapley_dict[\"rank\"+str(rank)].append(i)\n",
    "        if len(shapley_dict[\"rank\"+str(rank)])>0: \n",
    "            shapley_dict[\"rank\"+str(rank)]=shapley_dict_count(shapley_dict[\"rank\"+str(rank)])\n",
    "            if rank==level: \n",
    "                return shapley_dict,shapley_storage\n",
    "            else: \n",
    "                rank+=1 \n",
    "                # round+=1                       \n",
    "        else: \n",
    "            print(\"No results after the \"+str(round)+\" round\")\n",
    "        round+=1 \n",
    "        \n",
    "    if round==2: \n",
    "        shapley_dict[\"rank\"+str(rank)]=[]\n",
    "        for i in range(node_num):\n",
    "            # print(i)\n",
    "            for j in range(i): \n",
    "                graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i,j],add_sudo_nodes=False)    \n",
    "                shapley_i=shapley_storage[i,0,0]-model(graph_i)[2] \n",
    "                shapley_i=graph_label-model(graph_i)[2] \n",
    "                \n",
    "                # if (i==5) and (j==4): \n",
    "                #     print(shapley_storage[i,0,0],model(graph_i)[2],shapley_i)\n",
    "                if shapley_i!=0: \n",
    "                    shapley_storage[i,j,0]=1\n",
    "                    shapley_dict[\"rank\"+str(rank)].append(i)\n",
    "                    shapley_dict[\"rank\"+str(rank)].append(j)\n",
    "    \n",
    "        if len(shapley_dict[\"rank\"+str(rank)])>0: \n",
    "            shapley_dict[\"rank\"+str(rank)]=shapley_dict_count(shapley_dict[\"rank\"+str(rank)])\n",
    "            if rank==level: \n",
    "                return shapley_dict,shapley_storage \n",
    "            else: rank+=1 \n",
    "        else: \n",
    "            print(\"No results after the \"+str(round)+\" round\") \n",
    "        round+=1\n",
    "            \n",
    "    if round==3: \n",
    "        shapley_dict[\"rank\"+str(rank)]=[]\n",
    "        for i in range(node_num):\n",
    "            for j in range(i): \n",
    "                for k in range(j): \n",
    "                    graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i,j,k],add_sudo_nodes=False)    \n",
    "                    shapley_i=shapley_storage[i,j,0]-model(graph_i)[2] \n",
    "                    shapley_i=graph_label-model(graph_i)[2] \n",
    "                    # if shapley_i>0: \n",
    "                    if model(graph_i)[2]<1: \n",
    "                        shapley_storage[i,j,k]=1\n",
    "                        shapley_dict[\"rank\"+str(rank)].append(i)\n",
    "                        shapley_dict[\"rank\"+str(rank)].append(j)\n",
    "                        shapley_dict[\"rank\"+str(rank)].append(k)\n",
    "        if len(shapley_dict[\"rank\"+str(rank)])>0: \n",
    "            shapley_dict[\"rank\"+str(rank)]=shapley_dict_count(shapley_dict[\"rank\"+str(rank)])\n",
    "            if rank==level: \n",
    "                return shapley_dict,shapley_storage \n",
    "            else: rank+=1 \n",
    "        else: \n",
    "            print(\"No results after the \"+str(round)+\" round\") \n",
    "            round+=1\n",
    "        # return shapley_dict  , shapley_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.668623Z",
     "iopub.status.idle": "2024-05-08T06:42:31.670160Z",
     "shell.execute_reply": "2024-05-08T06:42:31.669736Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.669488Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_shapley_specified(graph,level=1,shapley_storage=None,skip_to_round=1,previous_shapley=None,node_list=node_lst): \n",
    "    graph_label=model(graph)[2]\n",
    "    node_num=len(node_list)\n",
    "    n=node_num\n",
    "    if shapley_storage==None: \n",
    "        shapley_storage = torch.zeros((n,n,n))\n",
    "    else: \n",
    "        shapley_storage=shapley_storage\n",
    "    shapley_dict={}\n",
    "    round=skip_to_round\n",
    "    rank=1\n",
    "\n",
    "    if round==1:     \n",
    "        shapley_dict[\"rank\"+str(rank)]=[]\n",
    "        for t in range(node_num):\n",
    "            # global round\n",
    "            # subset = torch.ones_like(graph.edge_index[0], dtype = bool)\n",
    "            # subset[[i]] = False\n",
    "            # graph_i = torch_geometric.data.Data(x=graph.x, #=th_delete(graph.x,[i]),\n",
    "            #                                     edge_index=torch_geometric.utils.subgraph(subset,graph.edge_index,relabel_nodes=True)[0],\n",
    "            #                                     y=graph.y)\n",
    "            i=node_list[t]\n",
    "            graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i],add_sudo_nodes=False)  \n",
    "            shapley_i=graph_label-model(graph_i)[2] \n",
    "            # print(shapley_i)\n",
    "            if shapley_i!=0: \n",
    "                shapley_storage[i,0,0]=1\n",
    "                shapley_dict[\"rank\"+str(rank)].append(i)\n",
    "        if len(shapley_dict[\"rank\"+str(rank)])>0: \n",
    "            shapley_dict[\"rank\"+str(rank)]=shapley_dict_count(shapley_dict[\"rank\"+str(rank)])\n",
    "            if rank==level: \n",
    "                return shapley_dict,shapley_storage\n",
    "            else: \n",
    "                rank+=1 \n",
    "                # round+=1                       \n",
    "        else: \n",
    "            print(\"No results after the \"+str(round)+\" round\")\n",
    "        round+=1 \n",
    "        \n",
    "    if round==2: \n",
    "        shapley_dict[\"rank\"+str(rank)]=[]\n",
    "        for t in range(node_num):\n",
    "            # print(i)\n",
    "            for s in range(t):\n",
    "                i=node_list[t]\n",
    "                j=node_list[s]\n",
    "                graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i,j],add_sudo_nodes=False)    \n",
    "                shapley_i=shapley_storage[i,0,0]-model(graph_i)[2] \n",
    "                shapley_i=graph_label-model(graph_i)[2] \n",
    "                \n",
    "                # if (i==5) and (j==4): \n",
    "                #     print(shapley_storage[i,0,0],model(graph_i)[2],shapley_i)\n",
    "                if shapley_i!=0: \n",
    "                    shapley_storage[i,j,0]=1\n",
    "                    shapley_dict[\"rank\"+str(rank)].append(i)\n",
    "                    shapley_dict[\"rank\"+str(rank)].append(j)\n",
    "    \n",
    "        if len(shapley_dict[\"rank\"+str(rank)])>0: \n",
    "            shapley_dict[\"rank\"+str(rank)]=shapley_dict_count(shapley_dict[\"rank\"+str(rank)])\n",
    "            if rank==level: \n",
    "                return shapley_dict,shapley_storage \n",
    "            else: rank+=1 \n",
    "        else: \n",
    "            print(\"No results after the \"+str(round)+\" round\") \n",
    "        round+=1\n",
    "            \n",
    "    if round==3: \n",
    "        shapley_dict[\"rank\"+str(rank)]=[]\n",
    "        for t in range(node_num):\n",
    "            for s in range(i): \n",
    "                for w in range(j): \n",
    "                    i=node_list[t]\n",
    "                    j=node_list[s]\n",
    "                    k=node_list[w]\n",
    "                    graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i,j,k],add_sudo_nodes=False)    \n",
    "                    shapley_i=shapley_storage[i,j,0]-model(graph_i)[2] \n",
    "                    shapley_i=graph_label-model(graph_i)[2] \n",
    "                    # if shapley_i>0: \n",
    "                    if model(graph_i)[2]<1: \n",
    "                        shapley_storage[i,j,k]=1\n",
    "                        shapley_dict[\"rank\"+str(rank)].append(i)\n",
    "                        shapley_dict[\"rank\"+str(rank)].append(j)\n",
    "                        shapley_dict[\"rank\"+str(rank)].append(k)\n",
    "        if len(shapley_dict[\"rank\"+str(rank)])>0: \n",
    "            shapley_dict[\"rank\"+str(rank)]=shapley_dict_count(shapley_dict[\"rank\"+str(rank)])\n",
    "            if rank==level: \n",
    "                return shapley_dict,shapley_storage \n",
    "            else: rank+=1 \n",
    "        else: \n",
    "            print(\"No results after the \"+str(round)+\" round\") \n",
    "            round+=1\n",
    "        # return shapley_dict  , shapley_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.671221Z",
     "iopub.status.idle": "2024-05-08T06:42:31.672696Z",
     "shell.execute_reply": "2024-05-08T06:42:31.671423Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.671403Z"
    }
   },
   "outputs": [],
   "source": [
    "graph=graph_test_1\n",
    "i=5\n",
    "j=4\n",
    "graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i,j],add_sudo_nodes=False)    \n",
    "shapley_i=1-model(graph_i)[2] \n",
    "shapley_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.673731Z",
     "iopub.status.idle": "2024-05-08T06:42:31.673960Z",
     "shell.execute_reply": "2024-05-08T06:42:31.673812Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.673805Z"
    }
   },
   "outputs": [],
   "source": [
    "def th_delete(tensor, indices):\n",
    "    mask = torch.ones(tensor.numel(), dtype=torch.bool)\n",
    "    mask[indices] = False\n",
    "    return tensor[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.675079Z",
     "iopub.status.idle": "2024-05-08T06:42:31.675279Z",
     "shell.execute_reply": "2024-05-08T06:42:31.675189Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.675182Z"
    }
   },
   "outputs": [],
   "source": [
    "def th_preserve(tensor, indices):\n",
    "    mask = torch.zeros(tensor.numel(), dtype=torch.bool)\n",
    "    mask[indices] = True\n",
    "    return tensor[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.677670Z",
     "iopub.status.idle": "2024-05-08T06:42:31.678685Z",
     "shell.execute_reply": "2024-05-08T06:42:31.678357Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.678333Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_with_color(graph,threshold=thres,isolates=True,exist_sudo=False,extra_node=False): \n",
    "    color_index_i=torch.bincount(graph.edge_index[0,:]).tolist()\n",
    "    if extra_node!=False: \n",
    "        color_index_i=[-1 if (i>extra_node) else color_index_i[i] for i in range(len(color_index_i)) ]\n",
    "    color_index_i=[i for i in color_index_i if i != 0]\n",
    "    print(color_index_i)\n",
    "    if extra_node==False: \n",
    "        color_index_i=[\"red\" if (i>thres) else \"lightblue\" for i in color_index_i]\n",
    "    if extra_node!=False: \n",
    "        color_index_i=[ \"yellow\" if (i==-1) else\"red\" if (i>thres) else \"lightblue\" for i in color_index_i]\n",
    "        # ['yes' if v == 1 else 'no' if v == 2 else 'idle' for v in l]\n",
    "    \n",
    "    g_i = torch_geometric.utils.to_networkx(graph)\n",
    "\n",
    "    if isolates==True: \n",
    "        g_i.remove_nodes_from(list(nx.isolates(g_i))) \n",
    "    if exist_sudo==True: \n",
    "        print(\"sudo\",graph.x.shape[0]-1)\n",
    "        g_i.remove_nodes_from([graph.x.shape[0]-1])\n",
    "        nx.draw(g_i,with_labels=True,node_color=color_index_i[:-1])\n",
    "    else: nx.draw(g_i,with_labels=True,node_color=color_index_i) # ,node_color=color_index_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.681143Z",
     "iopub.status.idle": "2024-05-08T06:42:31.681502Z",
     "shell.execute_reply": "2024-05-08T06:42:31.681393Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.681383Z"
    }
   },
   "outputs": [],
   "source": [
    "color_index_i=torch.bincount(graph_update.edge_index[0,:]).tolist()\n",
    "color_index_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.682608Z",
     "iopub.status.idle": "2024-05-08T06:42:31.683312Z",
     "shell.execute_reply": "2024-05-08T06:42:31.682848Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.682838Z"
    }
   },
   "outputs": [],
   "source": [
    "extra_node=10\n",
    "[color_index_i[i] if (i<extra_node) else \"yellow\" for i in range(len(color_index_i)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.685827Z",
     "iopub.status.idle": "2024-05-08T06:42:31.688139Z",
     "shell.execute_reply": "2024-05-08T06:42:31.688007Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.687983Z"
    }
   },
   "outputs": [],
   "source": [
    "color_index_i=[i for i in color_index_i if i != 0]\n",
    "color_index_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.690044Z",
     "iopub.status.idle": "2024-05-08T06:42:31.690575Z",
     "shell.execute_reply": "2024-05-08T06:42:31.690462Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.690453Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_test_1 = tu2[6]\n",
    "# graph_test_1 = tu2[7] #0,6,7,16,17,19,20,25,26,30,31,32\n",
    "# graph_test_1=new_subgraph(graph_test_1,add_node=[0,1,2,4,5,6,7])\n",
    "draw_with_color(graph_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.691453Z",
     "iopub.status.idle": "2024-05-08T06:42:31.691637Z",
     "shell.execute_reply": "2024-05-08T06:42:31.691557Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.691549Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_test_1 = tu2[7]\n",
    "draw_with_color(graph_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.692089Z",
     "iopub.status.idle": "2024-05-08T06:42:31.692217Z",
     "shell.execute_reply": "2024-05-08T06:42:31.692154Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.692147Z"
    }
   },
   "outputs": [],
   "source": [
    "shapley_add_node,shapley_add_node_storage=run_shapley(graph_test_1,level=2)\n",
    "# shapley_add_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.692861Z",
     "iopub.status.idle": "2024-05-08T06:42:31.693006Z",
     "shell.execute_reply": "2024-05-08T06:42:31.692936Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.692929Z"
    }
   },
   "outputs": [],
   "source": [
    "shapley_add_node_storage[5,4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.693518Z",
     "iopub.status.idle": "2024-05-08T06:42:31.693794Z",
     "shell.execute_reply": "2024-05-08T06:42:31.693690Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.693680Z"
    }
   },
   "outputs": [],
   "source": [
    "shapley_add_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.694151Z",
     "iopub.status.idle": "2024-05-08T06:42:31.694287Z",
     "shell.execute_reply": "2024-05-08T06:42:31.694222Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.694215Z"
    }
   },
   "outputs": [],
   "source": [
    "set(list(shapley_add_node['rank1'].keys())+list(shapley_add_node['rank2'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.694847Z",
     "iopub.status.idle": "2024-05-08T06:42:31.694974Z",
     "shell.execute_reply": "2024-05-08T06:42:31.694911Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.694904Z"
    }
   },
   "outputs": [],
   "source": [
    "node_lst=list(set( val for dic in  shapley_add_node.values() for val in dic.keys()))\n",
    "# seperate_1=new_subgraph(graph_test_1,list(set(list(shapley_add_node['rank1'].keys()))),draw_subgraph=True)\n",
    "seperate_1=new_subgraph(graph_test_1,node_lst,draw_subgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.695451Z",
     "iopub.status.idle": "2024-05-08T06:42:31.695642Z",
     "shell.execute_reply": "2024-05-08T06:42:31.695512Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.695506Z"
    }
   },
   "outputs": [],
   "source": [
    "cut_shapley=excl_low_value_nodes(shapley_add_node['rank2'],node_threshold=seperate_1.x.shape[0]*.5)\n",
    "cut_seperate_1=new_subgraph(graph_test_1,list(cut_shapley.keys()),draw_subgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.696167Z",
     "iopub.status.idle": "2024-05-08T06:42:31.696300Z",
     "shell.execute_reply": "2024-05-08T06:42:31.696235Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.696229Z"
    }
   },
   "outputs": [],
   "source": [
    "cut_shapley=excl_low_value_nodes(shapley_add_node['rank2'],excl_round=2,node_threshold=seperate_1.x.shape[0]*.3)\n",
    "cut_seperate_1=new_subgraph(graph_test_1,list(cut_shapley.keys()),draw_subgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.697357Z",
     "iopub.status.idle": "2024-05-08T06:42:31.697565Z",
     "shell.execute_reply": "2024-05-08T06:42:31.697498Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.697490Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_graph(node_num,max_degree,gseed,draw_graph=False,star=False):\n",
    "    np.random.seed(gseed)\n",
    "    if star==True: \n",
    "        degree_lst=np.ones(max_degree+1)\n",
    "        degree_lst[np.random.randint(max_degree)]=max_degree \n",
    "        print(degree_lst)\n",
    "    else: \n",
    "        degree_lst=np.random.randint(max_degree-3, size=node_num)\n",
    "        degree_lst[np.random.randint(node_num)]=max_degree \n",
    "    G = nx.random_degree_sequence_graph(degree_lst,gseed)\n",
    "    G = from_networkx(G)\n",
    "    G.x=torch.rand(node_num,1)\n",
    "    if draw_graph==True: \n",
    "        draw_with_color(G)\n",
    "    return G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.698040Z",
     "iopub.status.idle": "2024-05-08T06:42:31.698211Z",
     "shell.execute_reply": "2024-05-08T06:42:31.698110Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.698104Z"
    }
   },
   "outputs": [],
   "source": [
    "def seperate_graph(graph,node_to_excl,draw_graph=False): \n",
    "    edge_index_update=graph.edge_index\n",
    "    edge_index_update=sort_edge_index(edge_index_update)\n",
    "    label_max=graph.x.shape[0]\n",
    "    edge_index_id=(edge_index_update == node_to_excl).nonzero()\n",
    "    edge_index_copy=edge_index_update.clone()\n",
    "    node_degree=int((edge_index_id.shape[0])/2)\n",
    "    for i in range(node_degree): \n",
    "        edge_index_copy[0,edge_index_id[i,1]]=graph.x.shape[0]+i\n",
    "        edge_index_copy[1,edge_index_id[i+node_degree,1]]=graph.x.shape[0]+i\n",
    "    x_new=torch.cat((graph.x,torch.zeros(node_degree,1)))\n",
    "    graph_update=Data(x=x_new, edge_index=edge_index_copy)\n",
    "    if draw_graph==True: \n",
    "        draw_with_color(graph_update,extra_node=graph.x.shape[0])\n",
    "    return graph_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.699097Z",
     "iopub.status.idle": "2024-05-08T06:42:31.699267Z",
     "shell.execute_reply": "2024-05-08T06:42:31.699160Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.699154Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_graph(11,7,203,draw_graph=True,star=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.699831Z",
     "iopub.status.idle": "2024-05-08T06:42:31.699996Z",
     "shell.execute_reply": "2024-05-08T06:42:31.699922Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.699914Z"
    }
   },
   "outputs": [],
   "source": [
    "gg_star_1=generate_graph(8,7,203,draw_graph=True,star=True)\n",
    "gg_star_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.700747Z",
     "iopub.status.idle": "2024-05-08T06:42:31.701211Z",
     "shell.execute_reply": "2024-05-08T06:42:31.701128Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.701119Z"
    }
   },
   "outputs": [],
   "source": [
    "gg_star_1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.701649Z",
     "iopub.status.idle": "2024-05-08T06:42:31.701850Z",
     "shell.execute_reply": "2024-05-08T06:42:31.701712Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.701706Z"
    }
   },
   "outputs": [],
   "source": [
    "to_networkx(gg_star_1).nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.702377Z",
     "iopub.status.idle": "2024-05-08T06:42:31.702563Z",
     "shell.execute_reply": "2024-05-08T06:42:31.702455Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.702448Z"
    }
   },
   "outputs": [],
   "source": [
    "calculate_shapley(gg_star_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.703434Z",
     "iopub.status.idle": "2024-05-08T06:42:31.703727Z",
     "shell.execute_reply": "2024-05-08T06:42:31.703546Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.703538Z"
    }
   },
   "outputs": [],
   "source": [
    "gg_star_2=generate_graph(9,8,203,draw_graph=True,star=True)\n",
    "gg_star_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.704609Z",
     "iopub.status.idle": "2024-05-08T06:42:31.705293Z",
     "shell.execute_reply": "2024-05-08T06:42:31.705198Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.705187Z"
    }
   },
   "outputs": [],
   "source": [
    "calculate_shapley(gg_star_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.709355Z",
     "iopub.status.idle": "2024-05-08T06:42:31.713076Z",
     "shell.execute_reply": "2024-05-08T06:42:31.712944Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.712924Z"
    }
   },
   "outputs": [],
   "source": [
    "gg_star_2.edge_index[0]+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.714393Z",
     "iopub.status.idle": "2024-05-08T06:42:31.714771Z",
     "shell.execute_reply": "2024-05-08T06:42:31.714608Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.714562Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cat((gg_star_1.x,gg_star_2.x)),gg_star_1.x,gg_star_2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.715457Z",
     "iopub.status.idle": "2024-05-08T06:42:31.715689Z",
     "shell.execute_reply": "2024-05-08T06:42:31.715564Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.715556Z"
    }
   },
   "outputs": [],
   "source": [
    "edge_index_new_0=torch.cat((gg_star_1.edge_index[0],gg_star_2.edge_index[0]+8))\n",
    "edge_index_new_1=torch.cat((gg_star_1.edge_index[1],gg_star_2.edge_index[1]+8))\n",
    "edge_index_new=torch.stack((edge_index_new_0,edge_index_new_1))\n",
    "x_new=torch.cat((gg_star_1.x,gg_star_2.x))\n",
    "gg_star_3 = Data(x=x_new, edge_index=edge_index_new)\n",
    "draw_with_color(gg_star_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.720679Z",
     "iopub.status.idle": "2024-05-08T06:42:31.721157Z",
     "shell.execute_reply": "2024-05-08T06:42:31.720986Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.720968Z"
    }
   },
   "outputs": [],
   "source": [
    "calculate_shapley(gg_star_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.722580Z",
     "iopub.status.idle": "2024-05-08T06:42:31.722861Z",
     "shell.execute_reply": "2024-05-08T06:42:31.722738Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.722717Z"
    }
   },
   "outputs": [],
   "source": [
    "shapley_star_2,shapley_star_2_storage=run_shapley(gg_star_2,level=2)\n",
    "shapley_star_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.723782Z",
     "iopub.status.idle": "2024-05-08T06:42:31.725634Z",
     "shell.execute_reply": "2024-05-08T06:42:31.724948Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.724875Z"
    }
   },
   "outputs": [],
   "source": [
    "shapley_star_3,shapley_star_3_storage=run_shapley(gg_star_3,level=2)\n",
    "shapley_star_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.728636Z",
     "iopub.status.idle": "2024-05-08T06:42:31.728891Z",
     "shell.execute_reply": "2024-05-08T06:42:31.728786Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.728773Z"
    }
   },
   "outputs": [],
   "source": [
    "# shapley_star_3[\"rank1\"]/(gg_star_3.x.shape[0])\n",
    "a = {k: v / int(gg_star_2.x.shape[0])/(int(gg_star_2.x.shape[0])-1) for k, v in shapley_star_2[\"rank1\"].items()}\n",
    "a\n",
    "# b = {k: v *2/ int(gg_star_2.x.shape[0])/(int(gg_star_2.x.shape[0])-1)/(int(gg_star_2.x.shape[0])-2) for k, v in shapley_star_2[\"rank2\"].items()}\n",
    "# update_value=a.copy()\n",
    "# update_value=Counter(update_value)+Counter(b)\n",
    "# update_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.729955Z",
     "iopub.status.idle": "2024-05-08T06:42:31.730115Z",
     "shell.execute_reply": "2024-05-08T06:42:31.730042Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.730035Z"
    }
   },
   "outputs": [],
   "source": [
    "# shapley_star_3[\"rank1\"]/(gg_star_3.x.shape[0])\n",
    "a = {k: v / int(gg_star_2.x.shape[0]) for k, v in shapley_star_2[\"rank1\"].items()}\n",
    "b = {k: v *2/ int(gg_star_2.x.shape[0])/(int(gg_star_2.x.shape[0])-1) for k, v in shapley_star_2[\"rank2\"].items()}\n",
    "update_value=a.copy()\n",
    "update_value=Counter(update_value)+Counter(b)\n",
    "update_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.731277Z",
     "iopub.status.idle": "2024-05-08T06:42:31.731623Z",
     "shell.execute_reply": "2024-05-08T06:42:31.731439Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.731429Z"
    }
   },
   "outputs": [],
   "source": [
    "# shapley_star_3[\"rank1\"]/(gg_star_3.x.shape[0])\n",
    "a = {k: v / int(gg_star_3.x.shape[0])/(int(gg_star_3.x.shape[0])-1) for k, v in shapley_star_3[\"rank1\"].items()}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.733264Z",
     "iopub.status.idle": "2024-05-08T06:42:31.735179Z",
     "shell.execute_reply": "2024-05-08T06:42:31.733637Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.733627Z"
    }
   },
   "outputs": [],
   "source": [
    "a[12]-a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.736425Z",
     "iopub.status.idle": "2024-05-08T06:42:31.736592Z",
     "shell.execute_reply": "2024-05-08T06:42:31.736517Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.736509Z"
    }
   },
   "outputs": [],
   "source": [
    "# shapley_star_3[\"rank1\"]/(gg_star_3.x.shape[0])\n",
    "b = {k: v *2/ int(gg_star_3.x.shape[0])/(int(gg_star_3.x.shape[0])-1)/(int(gg_star_3.x.shape[0])-2) for k, v in shapley_star_3[\"rank2\"].items()}\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.737609Z",
     "iopub.status.idle": "2024-05-08T06:42:31.737887Z",
     "shell.execute_reply": "2024-05-08T06:42:31.737682Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.737675Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.739034Z",
     "iopub.status.idle": "2024-05-08T06:42:31.739522Z",
     "shell.execute_reply": "2024-05-08T06:42:31.739342Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.739312Z"
    }
   },
   "outputs": [],
   "source": [
    "update_value=a.copy()\n",
    "update_value=Counter(update_value)+Counter(b)\n",
    "update_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.741265Z",
     "iopub.status.idle": "2024-05-08T06:42:31.744598Z",
     "shell.execute_reply": "2024-05-08T06:42:31.743780Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.743764Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(list(update_value.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.745891Z",
     "iopub.status.idle": "2024-05-08T06:42:31.747384Z",
     "shell.execute_reply": "2024-05-08T06:42:31.747236Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.747223Z"
    }
   },
   "outputs": [],
   "source": [
    "shapley_star_3.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.748354Z",
     "iopub.status.idle": "2024-05-08T06:42:31.748507Z",
     "shell.execute_reply": "2024-05-08T06:42:31.748431Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.748425Z"
    }
   },
   "outputs": [],
   "source": [
    "# random.seed(12)\n",
    "# generate_graph(11,7,116,draw_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.749281Z",
     "iopub.status.idle": "2024-05-08T06:42:31.749997Z",
     "shell.execute_reply": "2024-05-08T06:42:31.749776Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.749743Z"
    }
   },
   "outputs": [],
   "source": [
    "# gg=generate_graph(11,7,116,draw_graph=True)\n",
    "gg=generate_graph(11,7,203,draw_graph=True,star=True)\n",
    "edge_index_update=gg.edge_index\n",
    "edge_index_update=sort_edge_index(edge_index_update)\n",
    "edge_index_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.750466Z",
     "iopub.status.idle": "2024-05-08T06:42:31.750625Z",
     "shell.execute_reply": "2024-05-08T06:42:31.750544Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.750538Z"
    }
   },
   "outputs": [],
   "source": [
    "seperate_graph(gg,4,draw_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.751245Z",
     "iopub.status.idle": "2024-05-08T06:42:31.751438Z",
     "shell.execute_reply": "2024-05-08T06:42:31.751318Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.751312Z"
    }
   },
   "outputs": [],
   "source": [
    "gg.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.753620Z",
     "iopub.status.idle": "2024-05-08T06:42:31.754100Z",
     "shell.execute_reply": "2024-05-08T06:42:31.753901Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.753889Z"
    }
   },
   "outputs": [],
   "source": [
    "dc={}\n",
    "dc[1]=0\n",
    "dc[1]+=.1\n",
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.756732Z",
     "iopub.status.idle": "2024-05-08T06:42:31.757324Z",
     "shell.execute_reply": "2024-05-08T06:42:31.757159Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.757145Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_shapley(graph,model=model): \n",
    "    phi={}\n",
    "    n=graph.x.shape[0] \n",
    "    x_lst=[i for i in range(n)]\n",
    "    w={}\n",
    "    for i in range(n): \n",
    "        w[i]=factorial(i)*factorial(n-i-1)/factorial(n)\n",
    "    for j in range(n): \n",
    "        lst_minus_j=x_lst.copy()\n",
    "        lst_minus_j.pop(j) \n",
    "        phi[j]=0\n",
    "        for k in range(n): \n",
    "            if k==0: \n",
    "                graph_C_j=new_subgraph(graph,include_node_lst=[j],exclude_node_lst=None,add_sudo_nodes=False)\n",
    "                v_C_j=model(graph_C_j)[2]\n",
    "                phi[j]+=w[k]*int(v_C_j)\n",
    "            else: \n",
    "                set_C_lst=list(itertools.combinations(lst_minus_j, k))\n",
    "                for l in range(len(set_C_lst)): \n",
    "                    set_C=list(set_C_lst[l])\n",
    "                    set_C_j=set_C+[j]\n",
    "                    graph_C=new_subgraph(graph,include_node_lst=set_C,exclude_node_lst=None,add_sudo_nodes=False)\n",
    "                    graph_C_j=new_subgraph(graph,include_node_lst=set_C_j,exclude_node_lst=None,add_sudo_nodes=False)\n",
    "                    v_C=model(graph_C)[2]\n",
    "                    v_C_j=model(graph_C_j)[2]\n",
    "                    phi[j]+=w[k]*int(v_C_j-v_C)\n",
    "    return phi \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.758011Z",
     "iopub.status.idle": "2024-05-08T06:42:31.758164Z",
     "shell.execute_reply": "2024-05-08T06:42:31.758085Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.758078Z"
    }
   },
   "outputs": [],
   "source": [
    "list(itertools.combinations(lst_minus_j, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.760344Z",
     "iopub.status.idle": "2024-05-08T06:42:31.760903Z",
     "shell.execute_reply": "2024-05-08T06:42:31.760741Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.760698Z"
    }
   },
   "outputs": [],
   "source": [
    "calculate_shapley(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T21:06:52.078503Z",
     "iopub.status.busy": "2024-03-19T21:06:52.075404Z",
     "iopub.status.idle": "2024-03-19T21:29:59.992785Z",
     "shell.execute_reply": "2024-03-19T21:29:59.991903Z",
     "shell.execute_reply.started": "2024-03-19T21:06:52.078304Z"
    }
   },
   "source": [
    "calculate_shapley(graph_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.763179Z",
     "iopub.status.idle": "2024-05-08T06:42:31.763982Z",
     "shell.execute_reply": "2024-05-08T06:42:31.763675Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.763376Z"
    }
   },
   "outputs": [],
   "source": [
    "n=gg.x.shape[0]\n",
    "x_lst=[i for i in range(n)]\n",
    "x_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.765751Z",
     "iopub.status.idle": "2024-05-08T06:42:31.766040Z",
     "shell.execute_reply": "2024-05-08T06:42:31.765920Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.765903Z"
    }
   },
   "outputs": [],
   "source": [
    "[1,2]+[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.767176Z",
     "iopub.status.idle": "2024-05-08T06:42:31.767705Z",
     "shell.execute_reply": "2024-05-08T06:42:31.767527Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.767501Z"
    }
   },
   "outputs": [],
   "source": [
    "factorial(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.768451Z",
     "iopub.status.idle": "2024-05-08T06:42:31.768645Z",
     "shell.execute_reply": "2024-05-08T06:42:31.768532Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.768525Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.771668Z",
     "iopub.status.idle": "2024-05-08T06:42:31.772924Z",
     "shell.execute_reply": "2024-05-08T06:42:31.772526Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.772511Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(itertools.combinations(x_lst, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.775453Z",
     "iopub.status.idle": "2024-05-08T06:42:31.776082Z",
     "shell.execute_reply": "2024-05-08T06:42:31.775955Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.775933Z"
    }
   },
   "outputs": [],
   "source": [
    "perm_lst=list(itertools.combinations(x_lst, 2))\n",
    "list(perm_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.776765Z",
     "iopub.status.idle": "2024-05-08T06:42:31.777891Z",
     "shell.execute_reply": "2024-05-08T06:42:31.776855Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.776848Z"
    }
   },
   "outputs": [],
   "source": [
    "x_j_lst=x_lst.copy()\n",
    "x_j_lst.pop(2)\n",
    "x_lst,x_j_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.779008Z",
     "iopub.status.idle": "2024-05-08T06:42:31.780062Z",
     "shell.execute_reply": "2024-05-08T06:42:31.779765Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.779713Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_i=new_subgraph(gg,include_node_lst=list(perm_lst[1]),exclude_node_lst=None,add_sudo_nodes=False)\n",
    "int(model(graph_i)[2]-model(gg)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.782670Z",
     "iopub.status.idle": "2024-05-08T06:42:31.783408Z",
     "shell.execute_reply": "2024-05-08T06:42:31.783182Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.783171Z"
    }
   },
   "outputs": [],
   "source": [
    "sort_edge_index(edge_index_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.786039Z",
     "iopub.status.idle": "2024-05-08T06:42:31.786355Z",
     "shell.execute_reply": "2024-05-08T06:42:31.786236Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.786228Z"
    }
   },
   "outputs": [],
   "source": [
    "gg.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.787850Z",
     "iopub.status.idle": "2024-05-08T06:42:31.788411Z",
     "shell.execute_reply": "2024-05-08T06:42:31.787952Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.787944Z"
    }
   },
   "outputs": [],
   "source": [
    "node_val=7\n",
    "label_max=gg.x.shape[0]\n",
    "edge_index_id=(edge_index_update == node_val).nonzero()\n",
    "edge_index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.789468Z",
     "iopub.status.idle": "2024-05-08T06:42:31.790968Z",
     "shell.execute_reply": "2024-05-08T06:42:31.790360Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.790330Z"
    }
   },
   "outputs": [],
   "source": [
    "edge_index_copy=edge_index_update.clone()\n",
    "node_degree=int((edge_index_id.shape[0])/2)\n",
    "print(node_degree)\n",
    "for i in range(node_degree): \n",
    "    # print((0,edge_index_id[i,1]),edge_index_copy[0,edge_index_id[i,1]])\n",
    "    # print((1,edge_index_id[i+node_degree,1]),edge_index_copy[1,edge_index_id[i+node_degree,1]])\n",
    "    edge_index_copy[0,edge_index_id[i,1]]=gg.x.shape[0]+i\n",
    "    edge_index_copy[1,edge_index_id[i+node_degree,1]]=gg.x.shape[0]+i\n",
    "edge_index_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.793881Z",
     "iopub.status.idle": "2024-05-08T06:42:31.794396Z",
     "shell.execute_reply": "2024-05-08T06:42:31.794271Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.794256Z"
    }
   },
   "outputs": [],
   "source": [
    "x_new=torch.cat((gg.x,torch.zeros(node_degree,1)))\n",
    "graph_update=Data(x=x_new, edge_index=edge_index_copy)\n",
    "draw_with_color(graph_update,extra_node=gg.x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.795387Z",
     "iopub.status.idle": "2024-05-08T06:42:31.795726Z",
     "shell.execute_reply": "2024-05-08T06:42:31.795548Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.795540Z"
    }
   },
   "outputs": [],
   "source": [
    "model(graph_update)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.798468Z",
     "iopub.status.idle": "2024-05-08T06:42:31.800818Z",
     "shell.execute_reply": "2024-05-08T06:42:31.800686Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.800668Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_shapley_mini(graph,level=1,shapley_storage=None,skip_to_round=1,previous_shapley=None,node_list=node_lst): \n",
    "    graph_label=model(graph)[2]\n",
    "    node_num=len(node_list)\n",
    "    n=node_num\n",
    "    if shapley_storage==None: \n",
    "        shapley_storage = torch.zeros((n,n,n))\n",
    "    else: \n",
    "        shapley_storage=shapley_storage\n",
    "    shapley_dict={}\n",
    "    round=skip_to_round\n",
    "    rank=1\n",
    "\n",
    "    if round==1:     \n",
    "        shapley_dict[\"rank\"+str(rank)]=[]\n",
    "        for t in range(node_num):\n",
    "            i=node_list[t]\n",
    "            edge_index_update=graph.edge_index\n",
    "            # print(graph)\n",
    "            graph_i=seperate_graph(graph=graph,node_to_excl=i)\n",
    "            # print(graph_i)\n",
    "            # graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i],add_sudo_nodes=False)  \n",
    "            shapley_i=graph_label-model(graph_i)[2] \n",
    "            # print(shapley_i)\n",
    "            if shapley_i!=0: \n",
    "                shapley_storage[i,0,0]=1\n",
    "                shapley_dict[\"rank\"+str(rank)].append(i)\n",
    "        if len(shapley_dict[\"rank\"+str(rank)])>0: \n",
    "            shapley_dict[\"rank\"+str(rank)]=shapley_dict_count(shapley_dict[\"rank\"+str(rank)])\n",
    "            if rank==level: \n",
    "                return shapley_dict,shapley_storage\n",
    "            else: \n",
    "                rank+=1 \n",
    "                # round+=1                       \n",
    "        else: \n",
    "            print(\"No results after the \"+str(round)+\" round\")\n",
    "        round+=1 \n",
    "        \n",
    "    # if round==2: \n",
    "    #     shapley_dict[\"rank\"+str(rank)]=[]\n",
    "    #     for t in range(node_num):\n",
    "    #         # print(i)\n",
    "    #         for s in range(t):\n",
    "    #             i=node_list[t]\n",
    "    #             j=node_list[s]\n",
    "    #             graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i,j],add_sudo_nodes=False)    \n",
    "    #             shapley_i=shapley_storage[i,0,0]-model(graph_i)[2] \n",
    "    #             shapley_i=graph_label-model(graph_i)[2] \n",
    "                \n",
    "    #             # if (i==5) and (j==4): \n",
    "    #             #     print(shapley_storage[i,0,0],model(graph_i)[2],shapley_i)\n",
    "    #             if shapley_i!=0: \n",
    "    #                 shapley_storage[i,j,0]=1\n",
    "    #                 shapley_dict[\"rank\"+str(rank)].append(i)\n",
    "    #                 shapley_dict[\"rank\"+str(rank)].append(j)\n",
    "    \n",
    "    #     if len(shapley_dict[\"rank\"+str(rank)])>0: \n",
    "    #         shapley_dict[\"rank\"+str(rank)]=shapley_dict_count(shapley_dict[\"rank\"+str(rank)])\n",
    "    #         if rank==level: \n",
    "    #             return shapley_dict,shapley_storage \n",
    "    #         else: rank+=1 \n",
    "    #     else: \n",
    "    #         print(\"No results after the \"+str(round)+\" round\") \n",
    "    #     round+=1\n",
    "            \n",
    "    # if round==3: \n",
    "    #     shapley_dict[\"rank\"+str(rank)]=[]\n",
    "    #     for t in range(node_num):\n",
    "    #         for s in range(i): \n",
    "    #             for w in range(j): \n",
    "    #                 i=node_list[t]\n",
    "    #                 j=node_list[s]\n",
    "    #                 k=node_list[w]\n",
    "    #                 # graph_i=new_subgraph(graph=graph,include_node_lst=None,exclude_node_lst=[i,j,k],add_sudo_nodes=False)    \n",
    "    #                 shapley_i=shapley_storage[i,j,0]-model(graph_i)[2] \n",
    "    #                 shapley_i=graph_label-model(graph_i)[2] \n",
    "    #                 # if shapley_i>0: \n",
    "    #                 if model(graph_i)[2]<1: \n",
    "    #                     shapley_storage[i,j,k]=1\n",
    "    #                     shapley_dict[\"rank\"+str(rank)].append(i)\n",
    "    #                     shapley_dict[\"rank\"+str(rank)].append(j)\n",
    "    #                     shapley_dict[\"rank\"+str(rank)].append(k)\n",
    "    #     if len(shapley_dict[\"rank\"+str(rank)])>0: \n",
    "    #         shapley_dict[\"rank\"+str(rank)]=shapley_dict_count(shapley_dict[\"rank\"+str(rank)])\n",
    "    #         if rank==level: \n",
    "    #             return shapley_dict,shapley_storage \n",
    "    #         else: rank+=1 \n",
    "    #     else: \n",
    "    #         print(\"No results after the \"+str(round)+\" round\") \n",
    "    #         round+=1\n",
    "    #     # return shapley_dict  , shapley_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.802004Z",
     "iopub.status.idle": "2024-05-08T06:42:31.802513Z",
     "shell.execute_reply": "2024-05-08T06:42:31.802387Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.802357Z"
    }
   },
   "outputs": [],
   "source": [
    "run_shapley(gg)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.803118Z",
     "iopub.status.idle": "2024-05-08T06:42:31.803272Z",
     "shell.execute_reply": "2024-05-08T06:42:31.803202Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.803194Z"
    }
   },
   "outputs": [],
   "source": [
    "run_shapley_mini(gg,level=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.803701Z",
     "iopub.status.idle": "2024-05-08T06:42:31.803907Z",
     "shell.execute_reply": "2024-05-08T06:42:31.803766Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.803759Z"
    }
   },
   "outputs": [],
   "source": [
    "seperate_1=new_subgraph(gg,list(run_shapley(gg)[0][\"rank1\"].keys()),draw_subgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.805421Z",
     "iopub.status.idle": "2024-05-08T06:42:31.807141Z",
     "shell.execute_reply": "2024-05-08T06:42:31.806300Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.806289Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils.convert import from_networkx\n",
    "sequence = [1, 2, 2, 3]\n",
    "G = nx.random_degree_sequence_graph(sequence)\n",
    "G = from_networkx(G)\n",
    "G.x=torch.rand(4,1)\n",
    "G.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.815933Z",
     "iopub.status.idle": "2024-05-08T06:42:31.827709Z",
     "shell.execute_reply": "2024-05-08T06:42:31.825999Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.825808Z"
    }
   },
   "outputs": [],
   "source": [
    "draw_with_color(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pablo Algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.833699Z",
     "iopub.status.idle": "2024-05-08T06:42:31.834123Z",
     "shell.execute_reply": "2024-05-08T06:42:31.833955Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.833942Z"
    }
   },
   "outputs": [],
   "source": [
    "fac_w=[]\n",
    "for k in range(1,n+1): \n",
    "    fac_w.append(factorial(n-k)*factorial(k-1)/factorial(n))\n",
    "fac_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.836148Z",
     "iopub.status.idle": "2024-05-08T06:42:31.836505Z",
     "shell.execute_reply": "2024-05-08T06:42:31.836316Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.836308Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(fac_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.839819Z",
     "iopub.status.idle": "2024-05-08T06:42:31.840886Z",
     "shell.execute_reply": "2024-05-08T06:42:31.840546Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.840529Z"
    }
   },
   "outputs": [],
   "source": [
    "def partial_shapley(graph,K=None): \n",
    "    n=graph.x.shape[0]\n",
    "    # print(n)\n",
    "    x_lst=[i for i in range(n)]\n",
    "    # print(n)\n",
    "    if K==None: \n",
    "        K=n\n",
    "    else: K=K \n",
    "    x_lst\n",
    "    fac_w=[]\n",
    "    for k in range(1,n+1): \n",
    "        fac_w.append(factorial(n-k)*factorial(k-1)/factorial(n))\n",
    "    fac_w\n",
    "    score_dict=dict(zip(x_lst,np.zeros(len(x_lst))))\n",
    "    score_dict\n",
    "    pos_G_k={}\n",
    "    for k in range(K): \n",
    "        # if k==1: \n",
    "        #     pos_G_k[k-1]=list(itertools.combinations(x_lst, k))\n",
    "        if k==0: \n",
    "            pos_G_k[k-1]=[]\n",
    "            pos_G_k[k]=[] \n",
    "            w_k=factorial(n-k-1)*factorial(k)/factorial(n)\n",
    "            for i in list(set(x_lst)): \n",
    "                # print(i)\n",
    "                U_i=[i]\n",
    "                graph_U_i=new_subgraph(graph,include_node_lst=list(set(x_lst)-set(U_i)),exclude_node_lst=None,add_sudo_nodes=False)\n",
    "                f_U_i=model(graph_U_i)[2] \n",
    "                \n",
    "                if int(f_U_i)==int(0): \n",
    "                    # print(f_U_i)\n",
    "                    # score_dict[i]=score_dict[i]+w_k\n",
    "                    score_dict[i]=score_dict[i]+w_k\n",
    "                else: \n",
    "                    pos_G_k[k].append(U_i)\n",
    "                    # print(U_i)\n",
    "        else: \n",
    "            # if k==1: \n",
    "            #     print(k)\n",
    "            #     print(pos_G_k[k-1])\n",
    "            pos_G_k[k]=[] \n",
    "            for U in pos_G_k[k-1]:\n",
    "                # print(list(U)) \n",
    "                U=list(U) \n",
    "                w_k=factorial(n-k-1)*factorial(k)/factorial(n)\n",
    "                for i in list(set(x_lst)-set(U)): \n",
    "                    # print(i)\n",
    "                    U_i=U+[i]\n",
    "                    # graph_U_i=new_subgraph(graph,include_node_lst=list(set(x_lst)-set(U_i)),exclude_node_lst=None,add_sudo_nodes=False)\n",
    "                    graph_U_i=new_subgraph(graph,include_node_lst=None,exclude_node_lst=U_i,add_sudo_nodes=False)\n",
    "                    f_U_i=model(graph_U_i)[2] \n",
    "                    \n",
    "                    if int(f_U_i)==int(0): \n",
    "                        # print(f_U_i)\n",
    "                        # score_dict[i]=score_dict[i]+w_k\n",
    "                        score_dict[i]=score_dict[i]+w_k\n",
    "                    else: pos_G_k[k].append(U_i)\n",
    "                \n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.842393Z",
     "iopub.status.idle": "2024-05-08T06:42:31.843050Z",
     "shell.execute_reply": "2024-05-08T06:42:31.842688Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.842673Z"
    }
   },
   "outputs": [],
   "source": [
    "draw_with_color(gg_star_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.844733Z",
     "iopub.status.idle": "2024-05-08T06:42:31.845246Z",
     "shell.execute_reply": "2024-05-08T06:42:31.844986Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.844973Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(gg_star_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.848761Z",
     "iopub.status.idle": "2024-05-08T06:42:31.849418Z",
     "shell.execute_reply": "2024-05-08T06:42:31.849088Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.849074Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(gg_star_2,K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.850858Z",
     "iopub.status.idle": "2024-05-08T06:42:31.851454Z",
     "shell.execute_reply": "2024-05-08T06:42:31.851032Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.851023Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(gg_star_2,K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.852435Z",
     "iopub.status.idle": "2024-05-08T06:42:31.852770Z",
     "shell.execute_reply": "2024-05-08T06:42:31.852677Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.852664Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(list(partial_shapley(gg_star_2).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.853631Z",
     "iopub.status.idle": "2024-05-08T06:42:31.853819Z",
     "shell.execute_reply": "2024-05-08T06:42:31.853733Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.853725Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(gg_star_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.856668Z",
     "iopub.status.idle": "2024-05-08T06:42:31.857775Z",
     "shell.execute_reply": "2024-05-08T06:42:31.857515Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.857494Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(gg_star_3,K=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.859410Z",
     "iopub.status.idle": "2024-05-08T06:42:31.859914Z",
     "shell.execute_reply": "2024-05-08T06:42:31.859676Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.859638Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(gg_star_3,K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.860826Z",
     "iopub.status.idle": "2024-05-08T06:42:31.861156Z",
     "shell.execute_reply": "2024-05-08T06:42:31.861016Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.861006Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(list(partial_shapley(gg_star_3,K=3).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.861891Z",
     "iopub.status.idle": "2024-05-08T06:42:31.863303Z",
     "shell.execute_reply": "2024-05-08T06:42:31.863093Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.863065Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(gg_star_3,K=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.866998Z",
     "iopub.status.idle": "2024-05-08T06:42:31.867848Z",
     "shell.execute_reply": "2024-05-08T06:42:31.867566Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.867549Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(list(partial_shapley(gg_star_3,K=4).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.870840Z",
     "iopub.status.idle": "2024-05-08T06:42:31.871247Z",
     "shell.execute_reply": "2024-05-08T06:42:31.871167Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.871157Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(gg_star_3,K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.872268Z",
     "iopub.status.idle": "2024-05-08T06:42:31.872586Z",
     "shell.execute_reply": "2024-05-08T06:42:31.872463Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.872449Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(gg_star_3,K=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.873856Z",
     "iopub.status.idle": "2024-05-08T06:42:31.874429Z",
     "shell.execute_reply": "2024-05-08T06:42:31.874219Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.874198Z"
    }
   },
   "outputs": [],
   "source": [
    "partial_shapley(graph_test_1,K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.875737Z",
     "iopub.status.idle": "2024-05-08T06:42:31.878572Z",
     "shell.execute_reply": "2024-05-08T06:42:31.878427Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.878408Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_test_2 = tu2[7]\n",
    "# draw_with_color(graph_test_1)\n",
    "partial_shapley(graph_test_2,K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.880411Z",
     "iopub.status.idle": "2024-05-08T06:42:31.880723Z",
     "shell.execute_reply": "2024-05-08T06:42:31.880532Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.880508Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(list(partial_shapley(gg_star_3,K=6).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.881923Z",
     "iopub.status.idle": "2024-05-08T06:42:31.883424Z",
     "shell.execute_reply": "2024-05-08T06:42:31.883010Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.882899Z"
    }
   },
   "outputs": [],
   "source": [
    "graph=graph_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.886710Z",
     "iopub.status.idle": "2024-05-08T06:42:31.887834Z",
     "shell.execute_reply": "2024-05-08T06:42:31.887729Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.887717Z"
    }
   },
   "outputs": [],
   "source": [
    "lst1=[1,2,3]\n",
    "lst2=np.zeros(len(lst1))\n",
    "dict(zip(lst1,lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.888589Z",
     "iopub.status.idle": "2024-05-08T06:42:31.888758Z",
     "shell.execute_reply": "2024-05-08T06:42:31.888660Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.888653Z"
    }
   },
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.889300Z",
     "iopub.status.idle": "2024-05-08T06:42:31.889480Z",
     "shell.execute_reply": "2024-05-08T06:42:31.889380Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.889372Z"
    }
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "n=graph.x.shape[0]\n",
    "x_lst=[i for i in range(n)]\n",
    "x_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.889901Z",
     "iopub.status.idle": "2024-05-08T06:42:31.890375Z",
     "shell.execute_reply": "2024-05-08T06:42:31.890245Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.890236Z"
    }
   },
   "outputs": [],
   "source": [
    "score_dict=dict(zip(x_lst,np.zeros(len(x_lst))))\n",
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.891433Z",
     "iopub.status.idle": "2024-05-08T06:42:31.893820Z",
     "shell.execute_reply": "2024-05-08T06:42:31.892919Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.892827Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_G_k={}\n",
    "pos_G_k[k-1]=list(itertools.combinations(x_lst, k))\n",
    "pos_G_k[k-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.901763Z",
     "iopub.status.idle": "2024-05-08T06:42:31.902624Z",
     "shell.execute_reply": "2024-05-08T06:42:31.902382Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.902361Z"
    }
   },
   "outputs": [],
   "source": [
    "int(f_U_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.908099Z",
     "iopub.status.idle": "2024-05-08T06:42:31.909990Z",
     "shell.execute_reply": "2024-05-08T06:42:31.909360Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.909317Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_G_k[k]=[]\n",
    "# k=n\n",
    "for U in pos_G_k[k-1]:\n",
    "    # print(list(U)) \n",
    "    U=list(U) \n",
    "    w_k=factorial(n-k)*factorial(k-1)/factorial(n)\n",
    "    for i in list(set(x_lst)-set(U)): \n",
    "        # print(i)\n",
    "        U_i=U+[i]\n",
    "        graph_U_i=new_subgraph(graph,include_node_lst=U_i,exclude_node_lst=None,add_sudo_nodes=False)\n",
    "        f_U_i=model(graph_U_i)[2] \n",
    "        \n",
    "        if int(f_U_i)==int(1): \n",
    "            # print(f_U_i)\n",
    "            score_dict[i]=score_dict[i]+w_k\n",
    "        else: pos_G_k[k].append(U_i)\n",
    "pos_G_k[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.911433Z",
     "iopub.status.idle": "2024-05-08T06:42:31.911870Z",
     "shell.execute_reply": "2024-05-08T06:42:31.911702Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.911693Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_U_i=new_subgraph(graph,include_node_lst=U_i,exclude_node_lst=None,add_sudo_nodes=False)\n",
    "draw_with_color(graph_U_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.912921Z",
     "iopub.status.idle": "2024-05-08T06:42:31.913391Z",
     "shell.execute_reply": "2024-05-08T06:42:31.913229Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.913217Z"
    }
   },
   "outputs": [],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.914739Z",
     "iopub.status.idle": "2024-05-08T06:42:31.916240Z",
     "shell.execute_reply": "2024-05-08T06:42:31.916063Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.916050Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.randint(3, 5, (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.917267Z",
     "iopub.status.idle": "2024-05-08T06:42:31.917511Z",
     "shell.execute_reply": "2024-05-08T06:42:31.917410Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.917401Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.921117Z",
     "iopub.status.idle": "2024-05-08T06:42:31.921839Z",
     "shell.execute_reply": "2024-05-08T06:42:31.921675Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.921644Z"
    }
   },
   "outputs": [],
   "source": [
    "seperate_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.923567Z",
     "iopub.status.idle": "2024-05-08T06:42:31.923801Z",
     "shell.execute_reply": "2024-05-08T06:42:31.923661Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.923653Z"
    }
   },
   "outputs": [],
   "source": [
    "run_shapley_specify(seperate_1[0],spec_node=list(seperate_1[1].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T06:42:31.924702Z",
     "iopub.status.idle": "2024-05-08T06:42:31.924998Z",
     "shell.execute_reply": "2024-05-08T06:42:31.924912Z",
     "shell.execute_reply.started": "2024-05-08T06:42:31.924904Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_rank(graph,levels=2): \n",
    "    shapley_dict[\"rank\"+str(levels)]=shapley_dict[\"rank1\"]\n",
    "    shapley_dict[\"rank1\"]=[]\n",
    "    # graph_with_sudo = new_subgraph(graph=graph,exclude_node_lst=[0],add_sudo_nodes=True)\n",
    "    for i in shapley_dict[\"rank\"+str(levels)]:\n",
    "        graph_i = new_subgraph(graph=graph,exclude_node_lst=[i],add_sudo_nodes=True)\n",
    "    # global round\n",
    "        # subset = torch.ones_like(graph_with_sudo.edge_index[0], dtype = bool)\n",
    "        # subset[[i]] = False\n",
    "        # # print(subset)\n",
    "        # # graph_i=torch_geometric.utils.subgraph(subset,graph_test_1.edge_index)\n",
    "        # graph_i = torch_geometric.data.Data(x=th_delete(graph_with_sudo.x,[i]),\n",
    "        #                                     edge_index=torch_geometric.utils.subgraph(subset,graph_with_sudo.edge_index,relabel_nodes=True)[0],\n",
    "        #                                     y=graph_with_sudo.y)\n",
    "        shapley_i=model(graph_i)[2] \n",
    "        # print(shapley_i)\n",
    "        if shapley_i>0: \n",
    "            print(i)\n",
    "            shapley_storage[i,:,:,:,:]=1\n",
    "            shapley_dict[\"rank\"+str(levels-1)].append(i)\n",
    "        if levels<=1:  \n",
    "            break \n",
    "        else: level=levels-1 \n",
    "shapley_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have seen the application of neural networks to graph structures. We looked at how a graph can be represented (adjacency matrix or edge list), and discussed the implementation of common graph layers: GCN and GAT. The implementations showed the practical side of the layers, which is often easier than the theory. Finally, we experimented with different tasks, on node-, edge- and graph-level. Overall, we have seen that including graph information in the predictions can be crucial for achieving high performance. There are a lot of applications that benefit from GNNs, and the importance of these networks will likely increase over the next years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=⭐&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider ⭐-ing our repository.    \n",
    "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=❔&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
